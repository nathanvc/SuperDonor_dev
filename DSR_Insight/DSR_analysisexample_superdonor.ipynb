{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import DSR_basicfn as bf\n",
    "import DSR_loadrawdata as ld\n",
    "import DSR_reformatdata as rf\n",
    "import DSR_sliderfigs as sf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import random\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import random\n",
    "import itertools as it\n",
    "import warnings\n",
    "from metric_learn import LMNN\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "\n",
    "#%matplotlib inline\n",
    "%matplotlib qt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Donor Sibling Registry (https://www.donorsiblingregistry.com) is a web site where people who were conceived via banked donor sperm can register information about their own sperm donor in order to locate half-genetic siblings. Over half of the donors listed on this site have only a single offspring listed, but 25% of sperm donors report having worked with more than one bank (the banks do not track this information). These duplicate donations, as well as transfers of samples between banks, can result in multiple distinct database listings that actually reflect the same underlying donor. In this project, I use machine learning techniques to detect distinct listings that may describe the same individual. The resulting search engine can be found at http://super-donor.com. This example notebook generates the database and metric-learning distance transformation that power the the search engine at super-donor. This notebook also references several modules for loading, cleaning and processing the data. These modules are located in this same github directory. Questions can be directed to me at nathan.the.vc@gmail.com. Work by Nathan Vierling-Claassen as part of the Bosgon Insight Health Data Science program, June/July 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, Loading, Pre-processing DSR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cryos\n",
      "Zygen\n",
      "Paces\n",
      "Idant\n",
      "FCCA\n",
      "Canam\n",
      "MSB\n",
      "PRS\n",
      "OHSU\n",
      "NoCASB\n",
      "ReproRes\n",
      "Reprolab\n",
      "Biogenetics\n",
      "Cryobio\n",
      "BosIVF\n",
      "Cryogen\n",
      "Xytex\n",
      "Fairfax\n",
      "CCB\n",
      "NECC\n",
      "Repromed\n",
      "TSBC\n",
      "RochReg\n",
      "ProTech\n",
      "Valley\n",
      "CryosNY\n",
      "Follas\n",
      "RepGerm\n",
      "IntCryo\n",
      "CryogamCO\n",
      "FertFirst\n",
      "Tyler\n",
      "NWCryo\n",
      "AnnArbor\n",
      "Midwest\n",
      "ESB\n",
      "HC\n",
      "AndroNW\n",
      "GG\n",
      "GeorgiaRS\n",
      "FINO\n",
      "EBMC\n",
      "RMC\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# Load and clean all raw data\n",
    "# ------------------\n",
    "# Raw data is contained in the file d in this same github repository\n",
    "# banklist is ordered list of individual bank keys (42 banks)\n",
    "# AllBanks is a dictionary of dictionaries, organized by bank and then by data type\n",
    "# Each data type is a list the length of the count of offspring for that bank\n",
    "# includes DonorID, FamilyID, UserID, Birthyear, Sex, DonorDesc, PostedBy, Bank\n",
    "d = '../DSR_rawdata_update/DSR_rawdata_6_6_2016.xlsx'\n",
    "(AllBanks, banklist) = ld.load_clean_allbanks(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrigIndex\n",
      "DonorDesc\n",
      "Bank\n",
      "UserID\n",
      "PostedBy\n",
      "FamilyID\n",
      "DonorID\n",
      "Sex\n",
      "Birthyear\n"
     ]
    }
   ],
   "source": [
    "# Useful cell if you want to remind yourself fieldnames, prints one bank at time\n",
    "for key in AllBanks['CCB']: print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Organize information by donor\n",
    "# -----------------\n",
    "# Generate dictionary with counts per donor,\n",
    "# designation of who posted, and description list\n",
    "# dictionary of dictionaries organized by bank and donor\n",
    "# (suppress meaningless runtime warnings from nanmean)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    Counts = rf.offsp_cnts(AllBanks, banklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Arrange count-per donor data, indicator for bank, list of counts, donor_ids, \n",
    "# and offsprint year into a list over all banks\n",
    "(allbanks_cnts, allbanks_bkind, list_allcnts, donorid_list, allbanks_offspyr) = rf.cnts_list(Counts, banklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Language preprocessing:\n",
    "# ---\n",
    "# Reformat description strings organized by donor,\n",
    "# split all entries on '. ', keep only unique entries (AllText key)\n",
    "# Dictionary of dictionaries, organized by bank and donorid\n",
    "# Pull fields that indicate some physical features (Weight, BloodType, Eye color),\n",
    "# some race/religion info (African or Black, Latino, Jewish)\n",
    "DescList = rf.desc_split(Counts, banklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine and clean descriptions for known pairs/groups of donor IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data for training data \n",
    "# Known distinct IDs that map to the same person\n",
    "# Confirmed by founder of the DSR who tracks this information\n",
    "groups = ld.load_groups('../DSR_rawdata_update/DSR_multibank_donors.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text to be removed from training set \"AllText\" field\n",
    "# Remove any field that directly indicates each donor's matched pair\n",
    "# (training on this won't help us find pairs that are not already known)\n",
    "rmv_library=['This donor is also',\n",
    "             'This is the same donor as',\n",
    "             'Same donor as PRS 143.',\n",
    "             'Also donated at',\n",
    "             'Also CCB #3204 and Zygen #236',\n",
    "             'Also donated at Paces Cryobank ID: PP133.',\n",
    "             'See also Fairfax Cryobank Donor',\n",
    "             'Also donated as #143 at Pacific Reproductive Services at the same time',\n",
    "             'Same donor as TSBC 832.',\n",
    "             'This donor 143 is the same as donor 832 at The Sperm Bank of California',\n",
    "             'Pacific Reproductive Services 3831 & California Cryobank 11444 is the same donor',\n",
    "             '11444--My donor also donated at California cryobank and this is his number',\n",
    "             'Pacific Reproductive Services 3831 & California Cryobank 11444 is the same donor',\n",
    "             'See also Fairfax Cryobank Donor',\n",
    "             'See also CCB(California Cryobank Donor #5039)',\n",
    "             'Also donated at Paces Cryobank in early',\n",
    "             'This donor was listed as Donor 1047 with ReproMed, Ltd',\n",
    "             'His sperm was available on the CLI Aug',\n",
    "             'This is a shared donor with Cryogenic Laboratories, Inc', \n",
    "             'See information and photos on that posting', \n",
    "             'His sperm was also available on the CLI Aug',\n",
    "             'In 2005 became donor 3267 through Pacific Reproductive Services.',\n",
    "             'See match on The Sperm Bank of California - 708.',\n",
    "             'Donor 5409 from CCB is also Donor 343 from Repro Lab in NY',\n",
    "             'Including Ca cryobank in Los Angeles',\n",
    "             'Prior sperm donations were also made through the Fertility Center, Santa Ana.',\n",
    "             'Note that this donor was also listed as Donor 1047 with ReproMed, Ltd',\n",
    "             'JUST found out through DNA matches on Ancestry.com and Gedmatch this donor may have donated to more banks than previously thought']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "CCB 3204\n",
      "['Height: 6\\'0\"', 'Electrical Engineer.', 'Brown Wavy hair.', 'Medium complexion', 'Height: 6\\'00\"', 'Computer Programmer/Engineer', 'Computer Programmer/Electrical Engineer', 'Born 1965.', \"Height: 6'0\", '', 'Weight: 165??', 'Brown Wavy hair', 'MS in Engineering', 'Blood type A+', 'German', 'Blue eyes', 'M.S', 'MS', 'Height: 6-0', 'Light Brown Wavy hair', 'Likes going to the beach,likes to travel,.', 'Born 1965', 'Weight: 165', 'Weight: 160', 'Born in Rapid City, South Dakota.']\n",
      "Cryogen 2281\n",
      "['Height: 6\\'0\"', '', 'MA Engineering/computer programming', 'Medium complexion', 'Engineer/Computer Programmer', 'Medium complexion.', 'German/German', \"Height: 6'0\", 'Brown Wavy hair', 'Height: 72\"', 'Blood type A+', 'German', 'Makes great babies!.', \"Height: 6'\", 'Blue eyes', 'Christian.', 'Computer Programmer.', 'Light Brown Wavy hair', 'Masters', 'Weight: 172', 'Height: 6.0', 'Born 1965', '', 'MSEE', 'Religion:Other', 'Weight: 160-170', 'Computer programmer']\n",
      "Zygen 236\n",
      "[\"Height: 6'0\", 'Height: 6', 'BSEE, MSEE', 'Brown Wavy hair', 'Light Brown Wavy hair', 'Blood type A+', 'German', 'Weight: 165', 'Medium complexion', 'Weight: 165-172', 'Favorite animal is dolphin, learned to play ukelele as a youth, donates to nature conservancy charities, raised Lutheran.', 'Blue eyes.', 'Computer Programmer', 'Born 1965', '', 'Blue eyes', 'Engineer', 'MS Engineering/Computers', 'Weight: 160-170', \"Height: 6'\", 'German/German']\n",
      "-----\n",
      "NECC D-5010\n",
      "['Catholic.', 'College-Cornell University', 'French - Sister (Neurologist) has twins.', 'Medium complexion', 'Degree in History?English', 'Customer Service', 'Weight: 164', 'Height: 6\\' 1\"', 'Medium complexion.', 'Undergrad degree from Cornell.', 'Height: 6\\'1\"', 'Blood type A+', 'Weight: 150', 'Supervisor Classical Music', 'Born 1970', 'Hazel eyes', 'Already described in previous posts.', 'Born Mar 1970', 'French', 'Weight: 170', 'Light Brown Straight hair', 'Graduated from Cornell University in History', 'Fair complexion', 'Blonde hair', 'Cornell University', 'Light Brown hair', 'Weight: 170 lbs', \"Height: 6'1''\", 'French.']\n",
      "BosIVF 5395\n",
      "[\"Adore's emersing in new cultures, new forms of architecture and new languages.\", 'Classical Music Dept', 'Catholic.', 'Light Brown Straight hair', 'Height: 6\\'1\"', 'Supv', 'History/English', 'French/French', 'Degree in History/English', 'College Degree in History/English', 'Roman Catholic.', 'Fair complexion', 'Weight: 164', 'Supervisor Classical Music', 'Height: 6\\' 1\"', 'Hazel eyes', 'Catholic', 'French']\n",
      "CCB 5027\n",
      "['Catholic.', 'Height: 6\\'1\"', 'Blonde Straight hair', 'Height: 168', 'Blood type A+', 'Weight: 165', 'Active writer, favorite sport- tennis, artistic abilities- active writer.', 'Weight: 165 lbs', 'Started donating Mar 1, 1994', 'University', 'BA', 'B.A', 'Fair complexion', 'Ancestry: French Born: Cambridge Massachusetts.', 'Born 1970', 'Caucasian', 'Hazel eyes', 'Catholic', 'Blonde Wavy hair', 'French']\n",
      "-----\n",
      "NECC D-5770\n",
      "['Born 1981', 'Student Aero/Astro Engineering', 'Height: 6\\'4\".', 'Height: 6\\'4\"', 'We are in Western Mass.', 'I would like them to have the opportunity to meet their family as they learn of them, rather than waiting until they turn 18.', 'Weight: 215', 'Brown Straight hair', 'We are interested in meeting other kids and their parents who used D5770', 'Medium complexion', 'Brown eyes', 'Dutch/Eastern European', 'I come from a very small family', 'BA Degree', 'I welcome contact with any of my childrens relatives including,half-siblings,donor himself,or family of the donor', 'MIT-Grad']\n",
      "Xytex AFN9626\n",
      "['Height: 6\\'5\"', 'Aerospace Engineering.', 'Height: 6\"5\"', 'French canadian', 'Brown eyes', 'Brown eyes.', 'Brown hair', 'Blood type O+']\n",
      "-----\n",
      "Xytex CIN9141\n",
      "['English, scottish, swish, irish', 'PHD', 'Medium complexion', 'College professor.', 'Born Oct 11, 1955', 'Blood type AB+', 'Height: 6\\'5\"', '', 'Ph.D in philosphy', 'Philosophy Professor', 'Working on PhD', 'Blue eyes', 'Weight: 180', 'Professor.', 'Light Brown Straight hair', '', 'Blonde Straight hair', 'Philosophy Professor.', 'English, Scottish, Swiss, Irish', 'Teacher', 'Philosophy professor.', 'English, Irish, Scottish, Swiss', 'PhD Philosophy', 'Fair complexion', 'Caucasian', 'Blonde hair', 'Ph.D', 'Blonde Wavy hair', \"Height: 6'5''\"]\n",
      "Paces PP133\n",
      "['Height: 6\\'5\"', '', 'Light Brown Straight hair', 'Teacher', 'English, Scottish, Swiss, Irish', 'Philosophy professor.', 'English, Irish, Scottish, Swiss', 'Ph.D', 'Medium complexion', 'Working on PhD', 'Blue eyes', 'Weight: 180', 'Born Oct 11, 1955', 'Blonde Wavy hair', 'Blood type AB+']\n",
      "-----\n",
      "PRS 580\n",
      "['German,irish,norwegian', 'Astrophysics PhD.', 'Height: 6\\'5\"', 'Blood type A+', 'Height: 6\"5\"', 'Blue eyes', 'Fair complexion.', 'Ph.D Astrophysics.', 'Light Brown Straight hair', 'Astrophysics student.', 'Astrophysics.', 'BS Astrophysics', \"Height: 6'5\", 'Fair complexion', 'Stopped donating Aug 2003', 'German/Irish/Norwegian', 'BA, PhD Astrophysics', 'PHD Astrophysics-student.', 'Light Brown hair', 'Ph.D', 'Weight: 230', 'German/Irish/Norweigian', 'Blood type B+', 'Born Jul 1971', 'Started donating Sep 2001']\n",
      "TSBC 1830\n",
      "['Comet Researcher', 'Astrophysics', 'Born Aug 1971', 'German, Irish, Norwegian', 'Brown Straight hair', 'German, Norwegian', 'Height: 6\\'6\"', 'Blue eyes', 'Comet researcher', 'Christian.', 'Weight: 228', 'Started donating Jan 1997', 'Caucasion', 'Born 1971', 'Light Brown Straight hair', 'Bachelors Science Astrophysics', 'Blue eyes.', 'Fair complexion', \"Bachelor's degree\", \"Height: 6'6\", 'Weight: 220', 'PhD Astrophysics', 'German and Irish', 'Light Brown hair', 'PhD', 'Blood type B+', 'Born Jul 1971', 'At time of donation, student/researcher.']\n",
      "-----\n",
      "CCB 1057\n",
      "['Height: 6\\'1\"and half', 'BA economics Berkeley.', 'Light Brown Wavy hair', 'Wavy hair', 'Weight: 195', 'Born 1963', 'Berkeley.', 'English/German', 'BA', 'Economics major', 'Fair complexion', 'Blue eyes', 'Protestant', 'Golden Key Honor Society, U.C', 'Blood type O+']\n",
      "TSBC 8\n",
      "['English/Welsh/Noregian', 'Blonde hair', 'Height: 6\\'1\"', 'Light Brown Straight hair', 'Blood type O-', 'German.', 'Started donating 1982', 'Born 1959', 'Weight: 200#', 'College-Economics', 'Blue eyes.', 'Fair complexion', 'Baptist.', 'Born Nov 11, 1959', 'Stopped donating 1985', 'Blue eyes', 'Caucasian']\n",
      "-----\n",
      "NECC D-237\n",
      "['White', 'Blonde Straight hair', 'Medical Student.', 'English, Irish', 'Blood type A+', 'Weight: 165', 'Blue eyes', 'Fair complexion.', 'M.D., Harvard Univ., Boston, MA', 'Height: 5\\'10\"', 'Born 1966']\n",
      "FCCA UJP65\n",
      "['Weight: 170', 'Light Brown Straight hair', 'Born Aug 1966', 'Medical Student', 'Loves all music and is athletically inclined; relaxed, with pleasant personality.', 'Blonde Straight hair', 'Blood type A+', 'Alot of siblings.', 'Medium complexion', \"Height: 5'10\", 'Height: 5\\' 10\"', 'Medical student', 'Irish', 'B.S', 'Weight: Thin', 'Blue eyes', 'Fair complexion', 'English/Irish', 'Catholic']\n",
      "Reprolab UJP65\n",
      "['Weight: 170', 'Born Aug 1966', 'Blonde Straight hair', 'Medical Student', 'Relaxed, with a pleasant personality.', 'Blood type A+', 'Medium complexion', 'Height: 5\\' 10\"', 'B.S', 'Blue eyes', 'English/Irish', 'Catholic']\n",
      "-----\n",
      "Fairfax 1084\n",
      "['English/Dutch', 'Green eyes', 'Blonde Straight hair', 'Dutch/English', 'Weight: 145 lbs', 'Several half siblings reported.', 'Weight: 145', 'Height: 5\\' 10\"', 'Fair complexion', 'Fair complexion.', 'Donor had a child (girl, age 7 at time of donation).', 'Height: 5\\'10\"', 'Blood type AB+']\n",
      "Canam 1084\n",
      "['Green eyes', 'Blonde Straight hair', \"Height: 5'10''\", 'Dutch/english', 'Fair complexion.', 'Weight: 180', 'Blood type AB+']\n",
      "-----\n",
      "Idant C358\n",
      "['Height: 6\\'0\"', \"Donor's father was a heavy smoker and had esophagial cancer.\", 'Jewish Polish', 'Weight: 178lbs', 'Medium complexion', \"Green eyes, 6' tall, Brown curly hair B+ blood type, Polish, 178 lbs, Medium complexion and build.\", 'Jewish', 'Weight: 185', 'Polish Jewish', \"Height: 6'\", 'Polish', 'College Grad', 'Green eyes', 'Biology Masters', 'Teacher', \"Donor's grandfather had cardiac disease\", 'Born 1961', 'Fair complexion', 'Jewish.', 'Brown Curly hair', 'Student', 'Weight: 178 lbs', 'BS Biology, seeking Masters', 'Blood type B+']\n",
      "CCB 536\n",
      "['Green eyes', 'Brown Wavy hair', 'Cauc; Russian; Polish', 'Teacher', 'Bio / Chemistry', '4 yrs college; B.A', 'Born Jun 17, 1961', \"Height: 6'\", 'Fair complexion', 'Weight: 180', 'Blood type B+', 'Jewish.']\n",
      "-----\n",
      "Fairfax 348\n",
      "['Height: 6-3', 'Weight: 210', 'B S Business', 'Favorite movies Blues Brothers.', 'Norway/Sweden', 'Christian', 'Business Admin/Marketing', 'Medium complexion', 'Swedish/Norwegian', 'Born 1973', 'Started donating Feb 1997', 'Graduate Student', 'Favorite animal- dog Favorite food - salmon Favorite car - jeep Favorite movie - blues brothers.', \"Height: 6'2\", 'Blue eyes', '', 'Blood type B+', 'Blonde Wavy hair', 'Height: 6\\'2\"', 'Christian.', 'Norwegian Swedish-Czech']\n",
      "CCB 5039\n",
      "['Blonde hair', 'MBA', 'Height: 6\\'3\"', 'Started donating Feb 1995', 'Medium complexion', 'Born 1973', 'Business Administration/Marketing', 'Religion:Other', 'Blue eyes', '', 'Blood type B+', 'Weight: 213', 'Stopped donating 1996', 'Swedish - Norwegian']\n",
      "-----\n",
      "PRS 3267\n",
      "['Weight: 175', 'Height: 5\\'9\"', 'Chiropractor.', 'Born 1976', 'BS Biology', 'Brown Straight hair', \"Height: 5'9\", 'Born Apr 1976', 'Started donating Mar 2005', 'Italian, German, English, French', 'Medium complexion', 'BS Biology, DC Chiropractic', 'Donor also donated at TSBC in 2000-2001', 'Caucasian', 'Hazel eyes', '3 prior pregnancies listed.', 'Chiropractor', 'Blood type O+']\n",
      "TSBC 2236\n",
      "['Reports having 2 boys in 2002 and 1 boy in 2003 as a result of his donations at TSBC', 'Weight: 175', '50% Italian, 25% German, rest English, Dutch, Irish', 'Height: 5\\'9\"', 'BS Biology, was in Chiropractic School', 'Stopped donating 2002', 'Brown Straight hair', 'Born Apr 1976', 'Medium complexion', '', 'Now Chiropractor', 'Hazel eyes', 'Started donating 2001', 'Blood type O+']\n",
      "-----\n",
      "PRS 143\n",
      "[\"Height: 6'1\", 'Catholic.', 'Swedish/Italian (Actually English/Dutch', 'Weight: 156', 'Advanced degree', 'Public policy/Public Administration', 'Born Feb 1966', 'Started donating 1990', \"Master's in Public Administration\", 'Height: 6\\'1\"', 'Blood type A+', 'Swedish/Italian', 'Started donating 1987', 'Roman Catholic.', 'Blue eyes', 'Born 1966', 'Masters Public Administration', 'Masters in Public Administration', 'Blonde Straight hair', 'Born Feb 15, 1964', 'Stopped donating 1993', 'White Swedish/Italian', 'Fair complexion', 'Public Relations/Retired', 'Legislative Assistant Deleware Assembly', '', 'Weight: 160', 'This donor is not willing to be identified or contacted.', 'Weight: 153', 'Catholic', '']\n",
      "TSBC 832\n",
      "['Weight: 156', 'Roman Catholic', 'Born Feb 1966', 'Started donating 1990', 'Also donated in Delaware prior to moving to California in 1990', \"Master's in Public Administration\", 'Height: 6\\'1\"', 'Blood type A+', 'Masters Degree in Public Administration', 'Swedish/Italian', 'PR Assistant', 'Started donating 1987', 'Blue eyes', 'Swedish/Italian (actually English/Dutch)', 'Blonde Straight hair', 'Born Feb 15, 1964', 'Stopped donating 1993', '', 'Fair complexion', 'Public Relations/Retired', 'Height: 6\" 1\"', 'Weight: 153lbs', 'This donor wishes to remain anonymous.', '', 'Stopped donating 1990']\n",
      "-----\n",
      "Fairfax 341\n",
      "['Brown Curly hair', 'Protestant.', 'Law Student', 'Medium complexion', '- Political Science, Law degree student', 'B.A', 'Weight: 187', 'Danish/Anglo Saxon', 'Hazel eyes', \"Height: 6'3\", 'Blood type O+']\n",
      "OHSU 9837\n",
      "['Height: 6\\'3\"', 'BA political science, JD', 'Protestant', 'Please read my current updated message.', 'Danish German English Scottish', 'Born in Michigan, grew up on Hazelnut orchard south of Portland, Oregon.', 'Attorney', 'Blood type O+', 'Born in Michigan', 'Raised on a hazelnut farm (hey, I thought they were called filberts?) south of Portland', 'Brown Straight hair', 'Weight: 190', 'Born 1970', 'Born 1970.', 'Hazel eyes', 'Danish, German, English, Scottish', 'Attorney - civil litigation', 'Fair complexion', 'Started donating 2003', 'Height: 6\\'-3\"', 'Nice dimples and nice eyes', 'Brown hair']\n",
      "-----\n",
      "TSBC 708\n",
      "['Born May 1967', 'Started donating 1990', 'Born May 23', 'Economics', 'Clinic said he was very funny and smart.', 'Music.', 'Blood type A+', 'B.A', 'B.S', \"Height: 5'11\", 'Blue eyes', 'Hazel eyes', 'Blonde Straight hair', 'Musician', 'Methodist.', 'Fair complexion', 'Height: 5\\'10\"', 'Blonde hair', 'Weight: 145', 'Weight: 128', 'Dutch, English', 'BA Economics, Graduate studies English']\n",
      "NoCASB 708\n",
      "['Blonde hair', 'Musician', 'Blood type A+', 'Weight: 128', 'Dutch, English', 'Born May 1967', 'Fair complexion', 'BA Economics, Graduate studies English', 'Methodist', '', 'Hazel eyes', 'Started donating 1990', 'Height: 5\\'10\"']\n",
      "-----\n",
      "Idant A197\n",
      "['Catholic.', 'Student', 'Height: Medium', 'Weight: Medium Build', 'Medium complexion', 'Political Science Major', 'Italian/Irish', 'Born 1964', 'Hazel eyes', 'Brown hair', 'Blood type O+']\n",
      "Fairfax 512\n",
      "['Bachelor degree', 'Catholic.', 'Marketing Manager.', \"Height: 5'10''\", 'Medium complexion', 'YOU: A devoted season ticket holder of baseball and basketball seats', 'Started donating 1990', 'US: Proud parents of a wonderful daughter born in May 2003', 'Blood type O+', 'Business Administration', 'Marketing Mgr', 'Hotel & Restaurant Mgmnt', 'Brown Wavy hair', 'We would like donor to contact us with updated medical history--no strings attached', 'Marketing Manager', 'Irish/Italian', 'Weight: 170-180', 'The bank may have given you the number 804', \"Your favorite holidays are Christmas and Valentine's Day\", 'Italian', 'We would like to meet her half-siblings', 'Stopped donating 2004', 'B.S', 'Weight: 170-175 lbs', 'Italian/ Irish', 'Weight: 180', 'M.S', 'Weight: 170', 'MBA', 'You love Nolan Ryan and Lou Gherig', 'Has 3 older sisters, 2 older brothers.', 'Started donating Nov 2001.', 'Hotel and Restaurant Management', 'Business management', 'Fair complexion', 'Italian/Irish', 'We also would like very much to know more about you-her donor', 'Fairfax has donation sites in TX, VA, and PA', 'Height: 5\\'10\"', 'HAD A DOG NAMED NOLAN!.', 'MS in Hotel Resturant Mgmt', 'WHY WE ARE HERE: Like many others on this site, we are looking to give our daughter some sense of her complete biological picture', 'Interests: Debating/Politics.', 'His dad was a Gynecologist and died at age 46 from lung cancer', 'Restaurant management', 'There are no strings attached, we are not looking for anything more than just some information, and to thank you for your wonderful gift.', \"Height: 5'10\", 'Marketing', 'Brown eyes', 'Height: 5\\' 10\"', 'Hotel & Restaurant Management', 'Italian-Irish', 'Italian, Irish', 'Blonde Wavy hair', 'Catholic', 'Brown hair']\n",
      "-----\n",
      "PRS 3831\n",
      "['also speaks spanish and farsi', '', 'Persian', 'Communications', 'Brown Wavy hair', 'Weight: 184', 'Speaks, reads and writes in Chinese and Japanese', 'Born Oct 1983', 'Started donating 2006', 'Brown eyes', '', '', 'Brown hair', 'Height: 5\"11']\n",
      "CCB 11444\n",
      "['also speaks spanish and farsi', 'Born Oct1983', 'Persian', '', 'Speaks, reads and writes in Chinese and Japanese', 'Born Oct 1983', 'Started donating 2006', '', 'Brown eyes', 'Brown hair', 'Started donating 2009']\n",
      "-----\n",
      "Cryogen 272\n",
      "['Brown Wavy hair', 'Height: 6\\'4\"', 'German & Scottish, Irish, English', 'Medium complexion', 'Brown eyes', 'Born Jan 1970', 'Weight: 190', 'German, Irish, English', 'Fair complexion', 'Dark Brown Curly hair', 'Christian.', \"Master's Degree in Public Health\"]\n",
      "ReproRes RR272\n",
      "['Brown Wavy hair', 'Height: 6\\'4\"', 'German, Scottish, Irish, English', 'Blood type A+', 'German & Scottish, Irish, English', 'MA in Public Health, BA in Latin American Studies', 'Medium complexion', 'Brown eyes', 'Born Jan 1970', 'Weight: 190', 'Dark Brown Curly hair', 'Fair complexion.', 'Christian.']\n",
      "-----\n",
      "Fairfax 2551\n",
      "['Brown Curly hair', 'Speaks Latin.', 'Brown Wavy hair', 'BA Criminal Justice', 'Criminal Justice.', 'Military/tutor', 'German Swiss/German', 'German', 'Christian', 'Medium complexion', 'Blue eyes.', 'Weight: 205', 'German-Swiss/German', \"Height: 6'2\", 'Blue eyes', 'Military/Tutor', 'Height: 6\\'2\"', 'Christian.', 'Blood type O+']\n",
      "Xytex BFM4892\n",
      "[\"Height: 6'1\", 'Catholic.', 'Weight: 195', 'Medium complexion', 'German, Swiss', 'Dark Brown Wavy hair', 'BA', 'Born Sep 21, 1985', 'Blue eyes', 'Blood type O+']\n",
      "-----\n",
      "CCB 900\n",
      "['Software engineer', 'Brown Wavy hair', 'Polish, Russian, American, Jewish', 'Also speaks some French', '', 'Born 1959', 'Medium complexion', 'Started donating May 1, 1992', 'Brown eyes', \"Height: 6'\", 'Blood type B-', 'Jewish', 'Ph.D Economics', 'Tennis, golf, softball, running, weight-training, guitar, piano, clarinet', 'Weight: 185 lb']\n",
      "Zygen 241\n",
      "['Software engineer', 'Height: 6\\'0\"', 'PhD Economics', 'Brown Wavy hair', 'Russian, American', 'Also speaks some French.', 'Post graduate; Ph.D', 'Born 1959', 'Medium complexion', 'Brown eyes', \"Height: 6'\", 'Blood type B-', 'Jewish', 'Weight: 198', 'Tennis, golf, softball, running, weight-training, guitar, piano, clarinet', 'Jewish.', 'Computer consultant', 'Polish, Russian American, Jewish']\n",
      "-----\n",
      "CCB 5409\n",
      "['Dimples Favorite Foods: Pizza and Cereal Favorite Color: Black Favorite animal: Dogs.', 'Irish/Scottish', 'Weight: 155', 'Brown Straight hair', '', 'Medium complexion', 'Born 1973', 'Marketing', 'Brown eyes', 'Catholic', 'Height: 5\\'10\"', 'Started donating 2001', 'Blood type O+']\n",
      "Reprolab 343\n",
      "['Irish/scottish', 'Weight: 155', 'Born Jul 1973', 'Medium complexion', 'BS/MArketing and Advertising', 'Brown eyes', 'Height: 5\\'10\"', 'Brown hair', 'Recording assistant.', 'Blood type O+']\n",
      "-----\n",
      "CCB 1250\n",
      "['Height: 6\\'0\"', 'German/Irish, French/Scottish', 'Medium complexion', 'Weight: 163', 'Studying Philosophy', \"Height: 6'0\", 'Weight: 162', 'Brown Straight hair', 'Started donating 1992', \"Height: 6'\", 'Blue eyes', 'Hazel eyes', 'Christian.', 'Caucasion', 'Born 1971', 'Light Brown Straight hair', 'Christian', 'Caucasian', 'College', 'He was studying Philosophy, and interested in a career in Politics.', 'Studying philosophy, planning to study law', 'Blood type O-', 'Started donating Feb 1992', 'Attended college', 'Was born in Wash', 'Intereted in Politics.', 'DC']\n",
      "TSBC 1258\n",
      "['Light Brown Straight hair', 'Blood type O-', 'Weight: 168', 'Born Apr 1971', 'Medium complexion', \"Height: 6'\", 'Hazel eyes', 'Christian.']\n",
      "-----\n",
      "Zygen 249\n",
      "['Weight: 170', 'Born 1971', 'UCLA', 'Blonde Straight hair', 'Swiss/Ger/Dutch', 'Student - Philosophy/Logic (Major) Sociology (Minor)', 'Fair complexion', \"Height: 6'0''\", 'Blue eyes', 'Christian.']\n",
      "PRS 645\n",
      "['Left handed', 'Started donating 1998', 'Swiss, Irish, Dutch', 'Swiss irish dutch', 'Blood type O+', 'Born Aug 1971', 'Started donating 1997 in New York', 'Started donating Nov 2008', 'Philosophy, Education', \"Height: 6'\", 'Blue eyes', 'Christian.', 'Height: 6\\' 0\"', 'Weight: 170', 'Swiss/Dutch/Irish', 'Educator', 'Blonde Straight hair', 'Philosophy, mathematics', 'Left handed.', 'Christian', \"Height: 6' 0''\", 'Fair complexion', 'Stopped donating May 1998', '', 'Caucasian', 'Blonde hair', 'A few years back prs stated to me the donor was part of the military and had gotten married, which is the reason why he stopped donating.', '']\n",
      "-----\n",
      "Xytex BFN9591\n",
      "['Scottish, German, English, Choctaw Indian', 'Height: 6\\'5\"', 'Psychologist.', 'Light Brown Straight hair', 'Born Sep 9, 1966', 'Born Aug 9, 1966', 'Foot size 15E', 'Scottish, German, English, Choctaw', 'PhD', 'Medium complexion', 'Also donated as Cryobiology #CB353.', 'Weight: 245', 'Height: 6;5\"', 'Blue eyes', 'Psychologist', 'Started donating 2001', 'Very athletic', 'Blood type O+']\n",
      "Cryobio CB353\n",
      "['BA, MA ,Phd', \"His message to the parents was very sweet, though I don't remember the exact words -- basically to take good care of this human life that was being created.\", 'Pursuing a PhD', 'Graduate degree', 'Medium complexion', 'He had been in graduate school for 3 years when he filled out his donation papers', 'Blood type O+', \"I don't have the listing anymore, but I believe my donor had large eyes (with laugh lines) and high cheekbones\", 'Choctaw/German', 'Psychology', 'Height: 6\\'6\"', 'Blue eyes', 'He loved music, and had a library of over 800 CDs', 'Light Brown Straight hair', 'He strove for personal excellence in everything he did', 'Height: 6\\' 6\"', 'As I remember, he had won a silver medal in a Russian contest in high school, and had played pro football before a knee injury ended his career', 'Started donating 1996', 'Student', 'Light Brown hair', 'Agnostic.', 'Weight: 230', 'German, Native American (Choctaw)', 'Weight: 230 lbs', 'German/American Indian', \"Height: 6'6\"]\n",
      "-----\n",
      "CCB 760\n",
      "[\"Height: 6'1\", 'Weight: 170', 'Brown Wavy hair', 'Light Brown complexion.', 'Blue eyes.', 'Blue eyes', 'Weight: 180', \"Height: 6'1''\", 'Brown hair', 'Blood type O+']\n",
      "IntCryo R333\n",
      "['Lawyer', 'Weight: 165', 'Law Degree', 'Medium complexion', 'Brown eyes', 'Donor was married at the time of his donation and a practicing attorney at that time, also.', 'Height: 5\\'10\"', 'Brown hair', 'Blood type O+']\n",
      "BosIVF 270\n",
      "['I think I was told he was a research assistant but I may not be correct', 'I think he was a student at BU.', 'Fair complexion', 'Brown Wavy hair', 'Blood type O+']\n",
      "-----\n",
      "TSBC 2789\n",
      "['Catholic.', 'Italian, Lithuanian, English', 'Born Mar 1980', 'Bachelors of Fine Arts in Photography', 'Blue eyes', 'Dark Brown Curly hair', 'Weight: 190', 'Blood type A-', 'Fair complexion', 'Height: 6\\'2\"']\n",
      "CCB 12362\n",
      "['Italian, Lithuanian, English', 'Donated at TSBC as #2789 and CCB as #12362.', 'Born Mar 1980', 'Bachelors of Fine Arts in Photography', 'Blue eyes', 'Dark Brown Curly hair', 'Weight: 190', 'Blood type A-', 'Fair complexion', 'Height: 6\\'2\"', 'Catholic']\n",
      "-----\n",
      "Xytex BGL5043\n",
      "['Outgoing, helpful to others, into construction.', 'Blood type A+', 'Height: 180cm', 'Cherokee, Cuban, Dutch,German', 'Born Sep 26, 1987', 'Medium complexion', 'Christian', 'Dark Brown Curly hair', 'Blue eyes', 'Weight: 63kg', 'Major in History', 'Stock Clerk/Student']\n",
      "FertFirst BGL5043\n",
      "['He describes himself as being outgoing', 'Helpful to others', 'Blood type A+', 'Weight: 63kb', 'Height: 180cm', 'Born Sep 26, 1987', 'Medium complexion', 'Majored in History', 'Christian', 'Dark Brown Curly hair', 'The donors brother also donates his number is BGL 5157 and there is cousins as well.', 'Dutch, German', 'Blue eyes', 'Stock Clerk/Student', 'Cheokee, Cuban']\n",
      "-----\n",
      "Repromed 1047\n",
      "['Height: 5\"10\"', 'BA, chemistry and accounting', 'Writer/tax preparer', '', '(Canada), for sure listed on the 1993 and 2002 lists', '1986 donor list (he may have donated before then), and he was available until March of 2002 when Fairfax purchased CLI.', 'Brown Straight hair', '(Minnesota)', 'German/Norwegian', 'Medium complexion', 'Brown eyes', '', '', 'Weight: 186', '', 'Blood type O+']\n",
      "Cryogen 1047\n",
      "['Catholic.', 'Telemarketer/Writer/Tax Preparation.', 'Original profile said mother is a tax preparer, father was a policeman (died from heart attack at the age of 62 on or before 9-5-97)', 'Speaks German as a second language', 'His list of geographical restrictions (there are probably more births in other areas-as there are three children who are not on this list that I know of, and I am not sure if all these restrictions resulted in live births): Twin Cities, MN, Milwaukee, WI, Sioux Falls, SD, Green Bay, WI, Duluth, MN, Rhinelander, WI, Fargo, ND, Grand Forks, ND, Eau Claire, WI, Brainerd, MN, Columbia, MO, Lebanon, NH, Hastings, MN, Marquette, MI, Marshfield, WI, Willmar, MN, Ames, IA, Chippewa Falls, WI, and Petoskey, MI', \"Height: 5'10''\", 'Medium complexion', 'Farsighted, one brother, three sisters', 'Bachelors in Chemistry, Vocational School for Accounting', 'Blood type O+', '', 'Brown Straight hair', '1986 donor list (he may have donated before then), and he was available until March of 2002 when Fairfax purchased CLI', 'Started donating 1987', '2008 health update has allergies to dust mites, cats, trees, and grasses listed', 'Describes himself as a walking dictionary who has never lost at Trivial Pursuit', 'Writer/tax preparer, update in 2008 says office manager', 'Christian.', 'Weight: 186', 'Weight in 2008 changed to 200, due to inactivity from a foot injury', 'German/Norwegian', 'Christian', 'Height: 5\\'10\"', 'Stopped donating 2000', '', '.', '(Canada), for sure listed on the 1993 and 2002 lists', 'Writer/Tax preparer', 'College degree: chemistry', 'Also changed to farsighted, his original profile said nearsighted', 'Brown eyes', 'BA', 'In 2008 he is still listed as single']\n"
     ]
    }
   ],
   "source": [
    "# Remove fields from all text description that indicate known pair \n",
    "# and visually inspect training data\n",
    "for j,p in enumerate(groups):\n",
    "    p_banks=[]\n",
    "    p_ids=[]\n",
    "    print('-----')\n",
    "    for i in range(len(p)):\n",
    "        p_bank = p[i][0].strip()\n",
    "        p_id = str(p[i][1])\n",
    "        print(p_bank, p_id)\n",
    "    \n",
    "        if bf.find(Counts[p_bank]['Unq_Donors'],p_id):\n",
    "                        \n",
    "            for i,a in enumerate(DescList[p_bank][p_id]['AllText']):\n",
    "                for rmv in rmv_library:\n",
    "                    if rmv in a:\n",
    "                        #print(a)\n",
    "                        DescList[p_bank][p_id]['AllText'][i]=''\n",
    "                        continue\n",
    "            print(DescList[p_bank][p_id]['AllText'])\n",
    "        \n",
    "        else:\n",
    "            print('*** Not in database ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull identifying features from text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -----\n",
    "# organize categorical indicator vectors for\n",
    "# race/ethnicity/religion, eye color, existence of description\n",
    "# Dictionary organized by bank, then contains binary vectors for each feature\n",
    "# -----\n",
    "# features that map to single field\n",
    "feat_cats = ['Jewish', 'AA', 'Latino', 'AllText']\n",
    "# multiple features read out from single field\n",
    "eye_cats = ['Blue', 'Green', 'Hazel', 'Brown']\n",
    "DescCat = rf.desc_cat(DescList, Counts, banklist, feat_cats, eye_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Arrange categorical data into big list of all entries over all banks\n",
    "# matches the index order of by donorid_list \n",
    "# Entry 1 for exists, 0 for does not\n",
    "# --------------\n",
    "allbanks_cat = rf.feat_list(DescCat, banklist, feat_cats + eye_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate indicator for whether eye color information is included\n",
    "eye_lab_sh = ['Bl', 'Gr', 'H', 'Br']\n",
    "(allbanks_eyecat, eye_lab) = rf.compile_cat(allbanks_cat, eye_cats, eye_lab_sh)\n",
    "\n",
    "eye_exist=[]\n",
    "for e in allbanks_eyecat:\n",
    "    if e == 0:\n",
    "        eye_exist.append(0)\n",
    "    if e > 0:\n",
    "        eye_exist.append(1)\n",
    "\n",
    "# add eye_exit to allbanks_cat:\n",
    "allbanks_cat['EyeExist']=eye_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse data for blood type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make dictionary organized by bank, that contains a list of bloodtypes for each donor \n",
    "Cont_Cat = rf.cont_cat(DescList, Counts, banklist, ['BloodType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of possible bloodtypes\n",
    "BloodDict=['A+', 'A-', 'AB+', 'AB-', 'B+', 'B-', 'O+', 'O-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# arrange all bloodtypes into single column, over all donors\n",
    "blood_clean={};\n",
    "blood_clean['BloodType']=[]\n",
    "for b in banklist:\n",
    "    for i in Cont_Cat[b]['BloodType']:\n",
    "        temp_blood = list(set(i))\n",
    "        for j in temp_blood:\n",
    "            if not bf.findincludes_list([j],BloodDict) or len(j)>4:\n",
    "                temp_blood.remove(j)\n",
    "        real_blood = ' '.join(t for t in temp_blood)\n",
    "        if real_blood == '':\n",
    "            real_blood = np.nan\n",
    "        blood_clean['BloodType'].append(real_blood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse donor weight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and parse weight data, pull from 'Weight' field in DescList\n",
    "# (also suppress meaningless nanmean RuntimeWarnings)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    weight_allbanks={}\n",
    "    for b in banklist:\n",
    "        weight_allbanks[b]={}\n",
    "        weight_allbanks[b]['Weight']=[]\n",
    "        test_wt=[]\n",
    "        for d in Counts[b]['Unq_Donors']: test_wt.append(DescList[b][d]['Weight'])\n",
    "        weight_allbanks[b]['Weight'] = rf.parse_weight(test_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all weights into one big vector across all banks\n",
    "allbanks_weight_concat = rf.feat_list(weight_allbanks, banklist, ['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify donors with large groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# identify donors with large groups\n",
    "super_dons= np.asarray(allbanks_cnts) > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make binary indicator list for large groups\n",
    "super_cnt = [i for i,x in enumerate(super_dons) if x == True]\n",
    "len(super_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange features parsed so far into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make original categories dataframe\n",
    "cats_df = pd.DataFrame.from_dict(allbanks_cat)\n",
    "# make blood dataframe\n",
    "blood_df = pd.DataFrame.from_dict(blood_clean)\n",
    "# make weights dataframe\n",
    "df_weight = pd.DataFrame(allbanks_weight_concat)\n",
    "# make ordered list of bank_names\n",
    "bank_names=[banklist[k] for k in allbanks_bkind]  \n",
    "# make dictionary that has donor info, offspring count, \n",
    "# offspring birth year, large group indicator & convert to dataframe\n",
    "donor_info={'bankid': bank_names,\n",
    "           'donorid': donorid_list,\n",
    "           'offspcnt': allbanks_cnts,\n",
    "           'offspyr': allbanks_offspyr,\n",
    "           'super': super_dons}\n",
    "df_don_specs=pd.DataFrame(donor_info)\n",
    "\n",
    "# concatenate all dataframes together into one\n",
    "donor_df=pd.concat([df_don_specs, cats_df, blood_df,df_weight],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Test data  from banks known to share samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sperm banks have shared samples. Using this information, we can generate a test data set by identifying intersecting IDs, and then discarding pairs with duplicate descriptions. Banks known to share samples are CryosNY and Cryos International, TSBC and NoCASB (name change), Heredity Choice and Repository for Germinal Choice,  European Sperm Bank USA and CCB, Can-Am Cryo and Fairfax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cryos_dup = set.intersection(set(Counts['CryosNY']['Unq_Donors']), set(Counts['Cryos']['Unq_Donors']))\n",
    "# Remove donors who don't have description at one or the other bank\n",
    "removeCryos=['2771FINN', '5524', '1703', 'UGGER', 'GREY']\n",
    "for k in removeCryos:\n",
    "    Cryos_dup.remove(k) \n",
    "# Remove donors who have completely duplicate descriptions\n",
    "ident_Cryos=['2552NILS', '6205DREW','DARIN','5586', '3268UFFE', '137']\n",
    "for k in ident_Cryos:\n",
    "    Cryos_dup.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TSBC_dup = set.intersection(set(Counts['TSBC']['Unq_Donors']), set(Counts['NoCASB']['Unq_Donors']))\n",
    "# Remove donors who don't have description at one or the other bank\n",
    "removeTSBC=['989', '1200']\n",
    "for k in removeTSBC:\n",
    "    TSBC_dup.remove(k)\n",
    "# remove those with totally identical listings\n",
    "ident_TSBC=['741']\n",
    "for k in ident_TSBC:\n",
    "    TSBC_dup.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ESB_dup = set.intersection(set(Counts['ESB']['Unq_Donors']), set(Counts['CCB']['Unq_Donors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Canam_dup = set.intersection(set(Counts['Canam']['Unq_Donors']), set(Counts['Fairfax']['Unq_Donors']))\n",
    "# Remove donors who don't have description at one or the other bank\n",
    "removeCanam=['1887', '2729', '2192', '2175', '2046', '1915', '1911']\n",
    "for k in removeCanam:\n",
    "    Canam_dup.remove(k)\n",
    "# remove totally identical listings\n",
    "ident_Canam=['2398']\n",
    "for k in ident_Canam:\n",
    "    Canam_dup.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC_dup = set.intersection(set(Counts['HC']['Unq_Donors']), set(Counts['RepGerm']['Unq_Donors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of duplicate IDs with bank info\n",
    "dup_input=[]\n",
    "dup_input.append([Cryos_dup, 'CryosNY', 'Cryos'])\n",
    "dup_input.append([TSBC_dup, 'TSBC', 'NoCASB'])\n",
    "dup_input.append([ESB_dup, 'ESB', 'CCB'])\n",
    "dup_input.append([Canam_dup, 'Canam', 'Fairfax'])\n",
    "dup_input.append([HC_dup, 'HC', 'RepGerm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to output pair listings for each entry in dup_input\n",
    "def makeduppairs(duplist, bank1, bank2):\n",
    "    pair_out=[]\n",
    "    for i in duplist:\n",
    "        p1 = (bank1, i)\n",
    "        p2 = (bank2, i)\n",
    "        ind_pr=[p1, p2]\n",
    "        pair_out.append(ind_pr)\n",
    "    return pair_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make listing of test-data pairs from shared samples, in same format as \"groups\" above\n",
    "test_pairs=[]\n",
    "for i in range(len(dup_input)):\n",
    "    test_pairs.extend(makeduppairs(dup_input[i][0], dup_input[i][1], dup_input[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "CryosNY 1421DANE\n",
      "['Stopped donating 2007', 'Blonde hair', 'Height: 180', 'Started donating Aug 2007', 'Caucassian', 'Danish/Scandinavian', 'Weight: 180 lbs', 'Blood type A+', 'Danish', 'Engineer.', 'Stopped donating Aug 2007', 'BS, Engineering.', 'Weight: 180', \"Height: 6'\", 'Fair complexion', 'Blue eyes', 'Fair complexion.', 'Weight: 6', 'Blonde Wavy hair', \"Height: 6 '0\", 'Born 1966']\n",
      "Cryos 1421DANE\n",
      "['Height: 6\\'0\"', 'Would love to hear from other siblings/parents and of course one day from the donor who gave the biggest gift ever, the gift of life', 'DANISH/SCANDANAVIAN', 'Weight: 180 lbs', 'Engineer.', 'Engineer', 'I have limited information on the donor as it was pre law change and UK do not supply as much information as other countries allow', 'Weight: 180 pounds', 'Please get in touch so I can share info to ensure a match X', 'Stopped donating 2007', \"Height: 6'0\", 'Donor is married with 2 sons.', 'Started donating Aug 2007', 'Stopped donating 2006', 'I have a wonderful 7 year old son from this donor', 'Blood type A+', 'BS, Engineering.', \"Height: 6'\", 'Blue eyes', 'Fair complexion.', 'Weight: 180', 'Building materials', 'Weight: 180lbs', 'Interests: Sports, music', 'Blonde Wavy hair', 'Born 1966', 'Height: 6', 'Blonde Straight hair', 'Danish/Scandinavian', 'Blue eyes.', 'Fair complexion', '(PLEASED TO UPDATE THAT I CAN CONFIRM THAT MY DONOR HAS BEEN PROVEN BY DNA TO BE DANE 1421).', 'The donor has children of his own and donated to Cyros in Denmark', 'Blonde hair', 'I imported to The Midland Fertility clinic in UK', 'Caucassian', 'Danish', 'Stopped donating Aug 2007', 'University', 'Religion:Other', 'Scandinavian', 'ENGINEER', \"Height: 6 '0\", 'I have a sure way of identification as the donor wrote a \"saying\" in with his profile']\n",
      "-----\n",
      "CryosNY 27ERIK\n",
      "['Graduated in 2002 with a Bachelor degree in electrical engineering.', 'Born 1979', 'Weight: 76 kg', 'BS Student (Engineering).', 'Light Brown hair', 'Dark Brown Straight hair', 'Caucasian-Scandinavian', 'Graduated in 2002 - bachelor in electrical engineering.', 'Brown eyes', 'Blood type A-', 'Fair complexion', 'Scandinavian', \"Height: 5'11''\", 'Caucasian', 'Caucasian/Scandinavian', 'Height: 182 cm', 'Weight: 167', 'Engineering.', 'Weight: 167 lbs']\n",
      "Cryos 27ERIK\n",
      "['Graduated in 2002 with a Bachelor degree in electrical engineering.', 'Started donating 1999', 'Born 1979', 'Weight: 76 kg', 'BS Student (Engineering).', 'Light Brown hair', \"Height: 5'11''\", 'Dark Brown Straight hair', 'Stopped donating 2003', 'Graduated in 2002 - bachelor in electrical engineering.', 'Brown eyes', 'Blood type A-', 'Fair complexion', 'Scandinavian', 'Caucasian/Scandinavian', 'Height: 182 cm', 'Weight: 167', 'Engineering.', 'Weight: 167 lbs']\n",
      "-----\n",
      "CryosNY 2508ELIF\n",
      "['Student (natural science).', 'Light Brown Straight hair', 'M', 'S', 'natuaral sciences', 'Has been in burkina faso in his university time and wanted to help suffering people in the third world.', 'Blood type A+', 'Danish', 'Height: 194 cm', 'Academic', 'Born 1974', 'Brown Straight hair', 'Brown eyes', 'Scandinavian', 'M.S', 'Weight: 77 kg']\n",
      "Cryos 2508ELIF\n",
      "['Student (natural science).', 'M', 'M.s.natural science', 'Weight: 77 kg/169 lbs', 'S', 'Dark Brown Straight hair', 'Blood type A+', 'Danish', 'Height: 194 cm', 'Academic', 'Born 1974', 'Brown Straight hair', 'Brown eyes', 'Worked in burkina faso for a while.', 'Fair complexion', 'Scandinavian', 'Weight: 77 kg']\n",
      "-----\n",
      "CryosNY 3836\n",
      "['BS Student Marine Engineer.', 'Brown hair', 'Blood type A+', 'Started donating Nov 1, 2004', 'Blue eyes']\n",
      "Cryos 3836\n",
      "['Brown hair', 'Weight: 87', 'Height: 183', 'Blood type A+', 'BS Student Marine Engineering.', 'Blue eyes', 'Caucasian', 'Started donating Nov 1, 2004']\n",
      "-----\n",
      "CryosNY 927STIG\n",
      "['Psychology.', 'Weight: 192 lb', 'Brown Straight hair', 'Danish', 'Fair complexion', 'Grey eyes.', 'Blue eyes', 'Born 1969', 'Height: 190', 'Blood type O+']\n",
      "Cryos 927STIG\n",
      "['Brown hair', 'Psychology.', 'Weight: 192 lb', 'Brown Straight hair', 'Danish', 'Blue eyes.', 'Fair complexion', 'Blue eyes', 'Born 1969', 'Height: 190', 'Blood type O+']\n",
      "-----\n",
      "CryosNY 786\n",
      "['Weight: 172lbs', 'Blonde Straight hair', 'Height: 6,2', 'Danish', 'Born 1975', 'Blue eyes', 'Fair complexion.', 'Blood type O+']\n",
      "Cryos 786\n",
      "['Blonde hair', 'Started donating 1999', 'Blonde Straight hair', 'Weight: 78 kg', 'Height: 190 cm', 'Height: 190 cm/ 6,23 ft', 'Blue eyes.', 'Born 1975', 'Fair complexion', 'Scandinavian', 'Blue eyes', 'Weight: 78 kg/ 172 lbs', 'Medical School.', 'Blood type O+']\n",
      "-----\n",
      "CryosNY 869TORE\n",
      "['Blonde hair', 'Extended profile.', 'Blood type A+', 'Born 1975', 'Scandinavian']\n",
      "Cryos 869TORE\n",
      "['Blonde hair', 'Danish', 'Blood type A+', 'Height: 175', 'Born 1975', 'Weight: 76', 'Extended profile.']\n",
      "-----\n",
      "CryosNY 3291ANTE\n",
      "['Blonde Straight hair', 'Born 1978', 'Weight: 183', 'Also known as 3291.', 'Blood type A+', 'Height: 6\\' 4\"', 'Fair complexion', 'Blue eyes', 'Caucasian/Scandinavian']\n",
      "Cryos 3291ANTE\n",
      "['Height: 6\\'4\"', 'Caucasian/Scandinavian', 'Height: 6-4', 'Weight: 183', 'Stopped donating 2006', 'Brown Straight hair', 'Blood type A+', 'Height: 6\\' 4\"', 'Blue eyes', 'Fair complexion.', 'No Religion (Atheist).', 'Blonde Straight hair', 'Light Brown Straight hair', 'Height: 1,93', 'Also listed with donor number 3951.', 'Fair complexion', 'Caucasian', 'Pre Med.', 'Student', 'Born 1978', 'Danish', 'Started donating 2001', 'Attending Medical School']\n",
      "-----\n",
      "CryosNY 4688\n",
      "['Weight: 154', '( From Cryos International, Aarhus, Danmark ).', 'Blood type A+', 'Skandinavian', \"Height: 6'2\", 'Anonymous and caucasian', 'Key account manager', 'Brown hair']\n",
      "Cryos 4688\n",
      "['Weight: 154', 'Anonymous and caucasian.', 'Height: 6`2', 'Blood type A+', 'Scandinavian', 'Key account manager', 'Brown hair']\n",
      "-----\n",
      "CryosNY GEIR\n",
      "['Scandinavian', 'Born 1976', 'Medical school', 'Height: 6\\'3\"', 'Blood type A+', 'Brown Straight hair', 'Medical student', 'Weight: 180', 'Fair complexion', 'Height: 6 \\'3\"', 'Blue eyes', 'Caucasian/Scandinavian', 'Elite athlete in track and field.', 'Training in medicine when donating.']\n",
      "Cryos GEIR\n",
      "['Height: 6\\' 3\"', 'Scandinavian', 'Training in medicine.', 'Born 1976', 'Medical school', 'Height: 6\\'3\"', 'Blood type A+', 'Brown Straight hair', 'Weight: 180', 'Fair complexion', 'Blue eyes', 'Caucasian/Scandinavian', 'Elite athlete in track and field.', 'Doctor']\n",
      "-----\n",
      "CryosNY 3949\n",
      "['Blonde hair', 'Blood type O-', 'Weight: 70 kg', 'Started donating Mar 21, 2005', 'Music teacher.', 'Height: 180 cm', 'Blue eyes', 'Born 1970']\n",
      "Cryos 3949\n",
      "['Blonde hair', 'Blood type O-', 'Weight: 70 kg', 'Started donating Mar 21, 2005', 'Music teacher.', 'Blue eyes', 'Born 1970', 'Height: 184 cm']\n",
      "-----\n",
      "CryosNY 821OLAF\n",
      "['Blonde hair', 'Student of Psychology.', 'Started donating Jun 25, 1999', 'Caucasian / scandinavian', 'Blood type A+', 'Stopped donating Aug 11, 2000', 'Weight: 65 kg', 'Born 1975', 'Height: 183 cm', 'Blue eyes']\n",
      "Cryos 821OLAF\n",
      "['Height: 6\\'0\"', 'Student of Psychology.', 'Protestant.', 'Business', 'Donors alias: Olaf.', 'Height:', 'PSD Psychology?', 'Brown Straight hair', 'Blood type A+', 'Caucasian / scandinavian', 'Stopped donating Aug 11, 2000', 'Weight: 143 lbs', 'Blue eyes', 'Blonde Straight hair', 'Masters', 'Christian', 'Blue eyes.', 'Height: 183 cm', 'Caucasian', 'Weight: 143', '/ 143 ibs', 'Blonde hair', 'Danish', 'Started donating Jun 25, 1999', 'Weight: 65 kg', 'Born 1975']\n",
      "-----\n",
      "CryosNY 782TIMM\n",
      "['Height: 178 (5\\'10\")', 'Studying Medicine', 'Started donating Jan 2004', 'Born 1976', 'Looking for ties with this donor, unique in that 782 \"TIMM\" is Vietnamese and lives/lived in Denmark-he has traveled to Saigon to meet up with past family there and dreams of giving something back to his birth country Vietnam.He was a medical student.', 'Black Straight hair', 'Medium complexion', 'Brown eyes', 'Vietnamese', 'Graduated high school 1994 US, Danish high school exam in 1996, Graduate university 2005 with PhD', 'Blood type B+', 'Weight: 76 (168 lbs)', 'Medical Student/Doctor']\n",
      "Cryos 782TIMM\n",
      "['Height: 178 (5\\'10\")', 'Studying Medicine', 'Started donating Jan 2004', 'Born 1976', 'Looking for ties with this donor, unique in that 782 \"TIMM\" is Vietnamese and lives/lived in Denmark-he has traveled to Saigon to meet up with past family there and dreams of giving something back to his birth country Vietnam.He was a medical student.', 'Black Straight hair', 'Medium complexion', 'Brown eyes', 'Vietnamese', 'Fair complexion', 'Graduated high school 1994 US, Danish high school exam in 1996, Graduate university 2005 with PhD', 'Blood type B+', 'Agnostic', 'Weight: 76 (168 lbs)', 'Medical Student/Doctor']\n",
      "-----\n",
      "CryosNY 997SVEN\n",
      "['Born 1960', 'Just email the donor sibling registry.', 'Blonde Straight hair', 'Weight: 141 lbs', 'Christian', 'If donor wishes to join, we are happy to pay the fee', 'Studying photography/Organ player', 'Blood type A-', 'Height:', 'Blue eyes', 'Fair complexion', 'Caucasian']\n",
      "Cryos 997SVEN\n",
      "['Born 1960', 'studying photography', 'Just email the donor sibling registry.', 'Blonde Straight hair', 'Weight: 141 lbs', 'He has two children of his own', 'Christian', 'If donor wishes to join, we are happy to pay the fee', \"Height: 5'6\", 'Allergic to grass-pollen', 'Blood type A-', 'Fair complexion', 'Blue eyes', 'Plays many instruments', 'Caucasian', 'He was in the airforce for 9 months', 'Organ player']\n",
      "-----\n",
      "CryosNY 1146\n",
      "['Scandinavian', 'Weight: 69kg', 'Blood type A+', 'Height: 178cm', 'Blue eyes.', 'Weight: 69 kg', 'Height: 178', 'Dark Brown hair', 'Weight: 69', 'Height: 178 cm', 'Blue eyes', 'Born 1967', 'Nordic', 'Brown hair']\n",
      "Cryos 1146\n",
      "['Scandinavian', 'Blood type A+', 'Weight: 69 kg', 'Dark Brown hair', 'Blue eyes.', 'Height: 178', 'Weight: 69', 'Height: 178 cm', 'Blue eyes', 'Born 1967', 'Brown hair']\n",
      "-----\n",
      "CryosNY 3088KNUT\n",
      "['Blonde Straight hair', 'Danish', 'Archiology.', 'Fair complexion', 'Blue eyes', 'Msc']\n",
      "Cryos 3088KNUT\n",
      "['Blonde hair', 'Danish', 'Blue eyes.']\n",
      "-----\n",
      "CryosNY 006ARNE\n",
      "['Scandinavian.']\n",
      "Cryos 006ARNE\n",
      "['Scandinavian.', 'Born 1977']\n",
      "-----\n",
      "CryosNY 672DANN\n",
      "['Weight: 170', 'Masters Degree', 'Blood type O-', 'Media', 'Brown Straight hair', 'Medium complexion', 'Caucasian, Danish', 'Brown eyes', 'Height: 6-1', 'Media Science.', 'Scandinavian', 'Height: 6\\' 1\"', 'Brown eyes.', 'Christian.', 'Master Degree', 'Born 1972', 'Brown hair']\n",
      "Cryos 672DANN\n",
      "['Journalist', \"Height: 6'1\", 'Weight: 170', 'Height: 6\\'1\"', 'Blood type O-', 'Speaks several languages, has one sister, both parents were teachers.', 'Dark Brown Straight hair', 'Danish', 'Brown Straight hair', 'Masters Degree Media Science', 'Medium complexion', 'Christian', 'Brown eyes', 'Caucasian', 'Christian.', 'Born 1972']\n",
      "-----\n",
      "CryosNY 3778\n",
      "['Blue eyes', 'Computer engineering.', 'Brown hair', 'Ms', 'Scandinavian']\n",
      "Cryos 3778\n",
      "['Brown hair', 'Blue eyes', 'Height: 192 cm', 'MSc Computer Engineering.', 'Scandinavian']\n",
      "-----\n",
      "CryosNY MATT\n",
      "['Brown Straight hair', 'Blood type A+', 'Oral Surgeon', 'Doctorate in Dental Surgery', 'Height: 6 feet', 'Fair complexion', 'LOOKING FOR VIALS!.', 'Blue eyes', 'Born 1982', 'Weight: 165 pounds']\n",
      "Cryos MATT\n",
      "['Brown Straight hair', 'Blood type A+', 'Oral Surgeon', 'LOOKING FOR VIALS.', 'Doctorate in Dental Surgery', 'Height: 6 feet', 'Fair complexion', 'Blue eyes', 'Born 1982', 'Weight: 165 pounds']\n",
      "-----\n",
      "CryosNY 3001JENS\n",
      "['Brown hair', 'Height: 6\\'1\" (185cm)', 'Student.', 'Born 1979', 'Blood type A+', 'Danish', 'Studied Masters Physical Chemistry at university', 'Graduated from high school in 1999', 'Has 1 younger brother and his mother is English.', 'Fair complexion', 'Blue eyes', 'Weight: 73 kg', 'Also known as \"JENS\"', 'Height: 187 cm', 'Blonde Curly hair', 'Weight: 175lbs (80kg)']\n",
      "Cryos 3001JENS\n",
      "['Brown hair', 'Height: 6\\'1\" (185cm)', 'Student.', 'Born 1979', 'Studied for an MSc in Physical Chemistry at university', 'Donor also known as \"Jens\"', 'Has one younger brother', 'Mother was English.', 'Blood type A+', 'Danish', 'Graduated from high school in 1999', 'Fair complexion', 'Blue eyes', 'Weight: 73 kg', 'Blonde Wavy hair', 'Height: 187 cm', 'Weight: 175lbs (80kg)']\n",
      "-----\n",
      "CryosNY 704TYGE\n",
      "['Blood type O-', \"Have recently found an extended profile on this donor that I'm happy to share.\", 'Brown Straight hair', 'Started donating Mar 11, 1997', 'Weight: 75 kg', 'Height: 180 cm', 'Fair complexion', 'Scandinavian']\n",
      "Cryos 704TYGE\n",
      "['I am happy to share this.', 'Blood type O-', 'Brown Straight hair', 'Weight: 75 kg', 'Started donating Mar 11, 1999', 'Height: 180 cm', \"I have recently found an extended profile on the donor from January 2004 which we didn't have at the time of conception\", 'Fair complexion', 'Scandinavian']\n",
      "-----\n",
      "TSBC 16\n",
      "['Degree', 'Blood type A+', 'English/Irish/German', 'Database analyst.', 'Height: 5\\'11\"', 'Fair complexion', 'Blue eyes', 'Weight: 180', 'Blonde Wavy hair', 'Born Jan 1953']\n",
      "NoCASB 16\n",
      "['Degree', 'Blood type A+', 'English/Irish/German', 'Database analyst.', 'Fair complexion', 'Blue eyes', 'Weight: 180', 'Height: 5\\' 11\"', 'Born Jan 1953', 'Blonde Wavy hair']\n",
      "-----\n",
      "TSBC 2001\n",
      "['Caucasian', 'Born 1951', 'Weight: 159.', 'Height: 5\\'8\"']\n",
      "NoCASB 2001\n",
      "['Born Jun 1951.']\n",
      "-----\n",
      "TSBC 708\n",
      "['Born May 1967', 'Started donating 1990', 'Born May 23', 'Economics', 'Clinic said he was very funny and smart.', 'Music.', 'Blood type A+', 'B.A', 'B.S', \"Height: 5'11\", 'Blue eyes', 'Hazel eyes', 'Blonde Straight hair', 'Musician', 'Methodist.', 'Fair complexion', 'Height: 5\\'10\"', 'Blonde hair', 'Weight: 145', 'Weight: 128', 'Dutch, English', 'BA Economics, Graduate studies English']\n",
      "NoCASB 708\n",
      "['Blonde hair', 'Musician', 'Blood type A+', 'Weight: 128', 'Dutch, English', 'Born May 1967', 'Fair complexion', 'BA Economics, Graduate studies English', 'Methodist', '', 'Hazel eyes', 'Started donating 1990', 'Height: 5\\'10\"']\n",
      "-----\n",
      "TSBC 8\n",
      "['English/Welsh/Noregian', 'Blonde hair', 'Height: 6\\'1\"', 'Light Brown Straight hair', 'Blood type O-', 'German.', 'Started donating 1982', 'Born 1959', 'Weight: 200#', 'College-Economics', 'Blue eyes.', 'Fair complexion', 'Baptist.', 'Born Nov 11, 1959', 'Stopped donating 1985', 'Blue eyes', 'Caucasian']\n",
      "NoCASB 8\n",
      "['Height: 6\\'1\"', 'Light Brown Wavy hair', 'Protestant.', 'Started donating 1982', 'Born Dec 1959', 'English/German', 'Weight: 200#', 'College-Economics', 'Fair complexion', 'Blue eyes']\n",
      "-----\n",
      "TSBC 355\n",
      "['Had never been a sperm donor before', 'Had never served in the military', 'Brown Wavy hair', 'He was the oldest of four children, with one brother and two sisters.', 'Born Oct 1953', 'Hazel eyes', 'Roman Catholic', 'Employed in house construction in 1987', 'Additional information from long chart provided on donor: he was single in 1987, age 33 at the time he completed the interview', 'Medium body type', 'Exercised regularly with non-contact sports; hang gliding, frisbee, volleyball, swimming', 'Had no children at time, but had been responsible for a pregnancy in 1971 (chart provided no further information on this pregnancy)', 'Completed college, degree in poli sci in 1976', 'Fair complexion', 'Poor vision; he was nearsighted and wore glasses', 'Weight: 150 #', 'English/Irish', 'Had a routine appendectomy in 1970', 'Height: 5\\'10\"', 'Blood type O+']\n",
      "NoCASB 355\n",
      "['Brown Wavy hair', 'Born Oct 1953', 'Hazel eyes', 'Employed in house construction in 1987', 'Height: 5\\' 10\"', 'Completed college, degree in poli sci in 1976', 'Fair complexion', 'Roman Catholic.', 'Weight: 150 #', 'English/Irish', 'Blood type O+']\n",
      "-----\n",
      "TSBC 1258\n",
      "['Light Brown Straight hair', 'Blood type O-', 'Weight: 168', 'Born Apr 1971', 'Medium complexion', \"Height: 6'\", 'Hazel eyes', 'Christian.']\n",
      "NoCASB 1258\n",
      "['Light Brown Straight hair', 'Blood type O-', 'Weight: 168', 'Born Apr 1971', 'Height: 6\"', 'Medium complexion.', 'Hazel eyes']\n",
      "-----\n",
      "ESB 7093\n",
      "['Blonde Straight hair', 'Blood type O-', 'Height: 6`5`', 'Philosophy Student', 'Fair complexion', 'Caucasian', 'Blue eyes', 'AKA \"Georg\".', 'Born 1982', 'Weight: 177 lbs']\n",
      "CCB 7093\n",
      "['Student', 'Blonde Straight hair', 'Blood type O-', 'Height: 6`5`', 'Aka \"Georg\".', 'Philosophy Student', 'Fair complexion', 'Caucasian', 'Blue eyes', 'Born 1982', 'Weight: 177 lbs']\n",
      "-----\n",
      "Canam 2181\n",
      "['See Fairfax #2181.', 'Blood type A+', 'Height: 5\\'11\"', 'Weight: 185', 'Fair complexion', 'Blue eyes', 'German/Finnish', 'Blonde Curly hair']\n",
      "Fairfax 2181\n",
      "['Blonde hair', 'Other pregnancies reported', 'Student in Anthrolopology', 'Well travelled and well read.', 'Mentions James Joyce and Ulysses in his audio', 'Blood type A+', 'German', 'German/Finnish', 'Medium complexion', 'BA ENGLISH', 'Height: 5\\'11\"', 'TEACHER', 'Caucasian', 'Blue eyes', 'Fair complexion.', 'Blonde Curly hair', 'Christian.', 'BA In English and French', 'Weight: 185', 'Blonde Wavy hair']\n",
      "-----\n",
      "Canam 1084\n",
      "['Green eyes', 'Blonde Straight hair', \"Height: 5'10''\", 'Dutch/english', 'Fair complexion.', 'Weight: 180', 'Blood type AB+']\n",
      "Fairfax 1084\n",
      "['English/Dutch', 'Green eyes', 'Blonde Straight hair', 'Dutch/English', 'Weight: 145 lbs', 'Several half siblings reported.', 'Weight: 145', 'Height: 5\\' 10\"', 'Fair complexion', 'Fair complexion.', 'Donor had a child (girl, age 7 at time of donation).', 'Height: 5\\'10\"', 'Blood type AB+']\n",
      "-----\n",
      "Canam 1763\n",
      "['Brown Wavy hair', 'Light Brown Wavy hair', 'Blood type O-', 'Protestant.', 'German', 'Fair complexion', 'Blue eyes', 'Christian.']\n",
      "Fairfax 1763\n",
      "['German', 'Protestant.', 'Fair complexion', 'Brown Wavy hair', 'Blue eyes']\n",
      "-----\n",
      "Canam 2918\n",
      "[\"Height: 6'0\", 'Green eyes', 'Blonde Straight hair', 'Norwegion', 'Weight: 184', 'Height: 6.0', 'Medium complexion', 'Finnish/norwegian', 'University-Bachelor of Engineering.', 'Masters Engineering.', 'Blood type O+']\n",
      "Fairfax 2918\n",
      "['Height: 6\\'0\"', 'Medium complexion', 'Engineer', 'Blood type O+', 'Finnish/Norwegian', \"Height: 6' 0\", \"Height: 6'0\", 'BA Engineering', 'Finnish', 'In the USMC, Favorite song is Beast of Burden.', 'Christian.', 'MA/Engineering', 'BA/Engineering', 'Masters Engineering.', 'Caucasion', 'Height: 6\\' 0\"', 'Height: 6', 'Green eyes', 'Blonde Straight hair', 'Student.', 'Donor 2918 is attractive with Nordic features: blond hair and gorgeous pale green eyes.', 'Height: 6.0', 'Christian', 'Seeking BA/ Engineering', 'Masters in engineering', 'Fair complexion', 'Caucasian', 'Masters in Engineering', 'MA/engineering', 'Green eyes.', 'Weight: 184lbs', 'Height: 6.0\"', 'Blonde hair', 'Student', 'Musical, Team Sports, Individual Sports, Craftsman, Creative/Artistic.', 'Finnish/Norwegian with sizzeled nordic features.', 'MA/ Engineering', 'Weight: 184', 'Finnish/Norwegian-Finnish']\n",
      "-----\n",
      "Canam 523\n",
      "['Blue eyes.', 'Black hair', 'Blood type O+']\n",
      "Fairfax 523\n",
      "['Message for the parents of 0523 children.', 'Black hair', 'BS Anthropology.', 'BS/Anthropology.', 'Black Straight hair', 'Blue eyes.', 'Scottish/Irish', 'Fair complexion', 'Blue eyes', 'Weight: 180', 'Height: 5\\'10\"', 'Height: 5-10', 'Blood type O+']\n",
      "-----\n",
      "Canam 78\n",
      "['Brown Wavy hair', 'Video Editor.', 'Welsh/Irish', 'Weight: 165', 'Brown eyes', 'Olive complexion', 'BS Communications', 'Height: 5\\'10\"', 'Blood type O+']\n",
      "Fairfax 78\n",
      "['Recording tech.', 'Height: 5 10', 'BS Communication', 'Welsh/irish', 'BS Communications', 'BS communications', '4 years College', 'Blood type O+', 'Video Editor', 'Brown Wavy hair', 'Video Editor.', 'Welsh / Irish', 'Started donating 1987', 'Olive complexion', 'Irish/Welsh', 'Stopped donating 1993', 'Allergic to large doses of penicillin.', 'Caucasian', 'Stopped donating 1989', 'Height: 5\\'10\"', 'Welsh', 'Has two other brothers', '4 yrs college', 'Sales Rep.', 'Welsh/Irish', 'Weight: 165', 'Height: 5\\' 10\"', 'Brown eyes', 'Brown hair.', 'Had separated shoulder at one time', 'Started donating 1989']\n",
      "-----\n",
      "Canam 2149\n",
      "['Height: 6\\'1\"', 'Brown Wavy hair', 'Lawyer', 'Medium complexion', 'Brown eyes', 'Height: 6.1', 'Weight: 180', 'Jewish.', 'Law', 'Blood type O+']\n",
      "Fairfax 2149\n",
      "['Brown Curly hair', \"Height: 6'1\", 'Height: 6\\'1\"', 'Brown Wavy hair', 'PHD', 'Lawyer', 'German Jewish/Russian Jewish', 'RUSSIOAN jEWISH', 'Medium complexion', 'Attorney.', 'Brown eyes', \"Height: 6'?\", 'Height: 6\\'1\".', 'Jewish', 'Height: 6.1', 'Weight: 180', 'Jewish.', 'Blood type O+']\n",
      "-----\n",
      "Canam 2505\n",
      "['Weight: 210', 'Blood type A+', 'Irish/German', 'Attorney.', 'Juris Doctorate', 'Fair complexion', 'Blue eyes', 'Blonde Wavy hair', \"Height: 6'3\"]\n",
      "Fairfax 2505\n",
      "['Catholic.', 'Weight: 210', 'Lawyer', 'Height: 6\\'3\"', 'Law Degree', 'Height: 6ft 3 inches', 'Irish, German', 'Law degree', 'Attorney', 'PH-D', 'Blood type A+', 'Irish/German', 'Started donating Feb 1, 2006', 'Blue eyes', 'Fair complexion.', 'German/irish', 'Originally art director; law student', 'Juris Doctorate', 'Fair complexion', 'Blonde Curly hair', \"Height: 6'3\", 'College', 'Stopped donating Feb 2, 2006', 'Attorney.', 'German/Irish', 'Blonde Wavy hair']\n",
      "-----\n",
      "Canam 1558\n",
      "[\"Height: 6'1\", 'Student', 'Blonde Straight hair', 'Blood type A+', 'Irish/German', 'Irish German', 'Brown eyes', 'Fair complexion', 'Weight: 190', 'Height: 6.1', 'Fair complexion.', 'Wears glasses, Grandparents were 70,72,75 & 76 years old.', 'Computer Science']\n",
      "Fairfax 1558\n",
      "[\"Height: 6'1\", 'Medium complexion', 'Wears glasses', 'College Student', 'Height: 6\\'1\"', 'Computer science.', 'Blood type A+', 'Irish/German', 'Weight: 190', 'Fair complexion.', 'Caucasion', 'Blonde Straight hair', 'Caucasian Irish/German', 'Student in Computer Science.', 'Fair complexion', 'Grandparents were 70, 72, 75 & 76 years old.', 'Caucasian', 'Freshmen Computer Science.', 'Student', 'College graduate', 'No longer available as of 2006.', 'Irish German', 'Brown eyes', 'Was on the donor list as early as 3/92', 'Computer Science']\n",
      "-----\n",
      "Canam 1848\n",
      "['Height: 5\\'9\"', 'Scottish/German', 'Scientist.', 'Weight: 178', 'PhD Ops Research', 'Blue eyes', 'Blood type B+', 'Red Wavy hair']\n",
      "Fairfax 1848\n",
      "['Unitarian.', 'Weight: 170', 'Other.', 'Height: 5\\'9\"', 'Scottish/German', 'Scientist.', 'Weight: 178', 'Scottish, German', 'Scientist', 'PhD Ops Research', 'Blue eyes.', 'German, Scottish', 'PhD', 'Medium complexion', 'German/Scottish', 'Fair complexion', 'Blue eyes', 'Operations Research, USAF', 'Blood type B+', 'Phd Ops Research', 'Red Wavy hair']\n",
      "-----\n",
      "HC GREEN-RED\n",
      "['Music: evidence of good ability Athletics: in college he was a champion in a demanding sport Offspring: show great promise in music and exceptional physical coordination.', 'Northwest European', 'Blood type A+', 'Weight: 163', 'Fair complexion', 'Blue eyes', 'Professor of mathematics', 'Height: 5\\'10\"', 'Blonde Curly hair']\n",
      "RepGerm GREEN-RED\n",
      "['Northwest European', 'Blood type A+', 'Weight: 163', 'Fair complexion', 'Blue eyes', 'Professor of mathematics', 'Music :evidence of good ability In colleage he was a champion in a demanding sport Offspring show exceptional physical coordination and good in music.', 'Height: 5\\'10\"', 'Blonde Curly hair']\n"
     ]
    }
   ],
   "source": [
    "# Clean these pairs as above for the \"groups\" training set\n",
    "for j,p in enumerate(test_pairs):\n",
    "    p_banks=[]\n",
    "    p_ids=[]\n",
    "    print('-----')\n",
    "    listset=[]\n",
    "    for i in [0,1]:\n",
    "        p_bank = p[i][0].strip()\n",
    "        p_id = str(p[i][1])\n",
    "        print(p_bank, p_id)\n",
    "    \n",
    "        if bf.find(Counts[p_bank]['Unq_Donors'],p_id):\n",
    "                        \n",
    "            for i,a in enumerate(DescList[p_bank][p_id]['AllText']):\n",
    "                for rmv in rmv_library:\n",
    "                    if rmv in a:\n",
    "                        DescList[p_bank][p_id]['AllText'][i]=''\n",
    "                        continue\n",
    "                        \n",
    "            print(DescList[p_bank][p_id]['AllText'])\n",
    "            \n",
    "            listset.append(set(DescList[p_bank][p_id]['AllText']))\n",
    "            \n",
    "        else:\n",
    "            print('*** Not in database ***')\n",
    "            \n",
    "    if len((listset[0]-listset[1]))==len(listset[1]-listset[0])==0:\n",
    "        print(\"*** Descriptions Identical -- cut ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make bag-of-words including nouns & adjectives for each donor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define stem function\n",
    "porter=PorterStemmer()\n",
    "def stemfn(textlist):\n",
    "    return[porter.stem(word) for word in textlist]\n",
    "\n",
    "# remove words already categorized or likely meaningless in terms of match\n",
    "remove_library= ['blue', 'hazel', 'green', 'brown', 'aa', 'latino', 'jewish', 'blood', \n",
    "                 'weight', 'weight', 'height', 'eye', 'cryo', 'eye', 'fairfax', 'fertil', \n",
    "                 'mom', 'dad', 'offspring']\n",
    "\n",
    "# add blood and banklist names to the library of words to remove\n",
    "bloodlr=[b.lower() for b in BloodDict]                 \n",
    "banklr=[b.lower() for b in banklist]\n",
    "remove_library.extend(bloodlr)\n",
    "remove_library.extend(banklr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to pull and tag nouns & adjectives for a single entry from \"AllText\"\n",
    "# input list of strings (from DescList[b][d]['AllText'])\n",
    "def pullnouns(listin):\n",
    "    listout=[]\n",
    "    listin=' '.join(listin)\n",
    "    type(listin)\n",
    "    listin = listin.replace('/',' ')\n",
    "    listin = listin.replace('-',' ')\n",
    "    listin = listin.replace('.','')\n",
    "    text = nltk.word_tokenize(listin)\n",
    "    tags = nltk.tag.pos_tag(text)\n",
    "    for t in tags:\n",
    "        if t[1]=='NN' or t[1]=='NNP' or t[1]=='NNS' or t[1]=='NNPS' or t[1]=='JJ'or t[1]=='JJS' or t[1]=='JJR':\n",
    "            nn=t[0]\n",
    "            nn=nn.lower()\n",
    "            for rl in remove_library:\n",
    "                if rl in nn:\n",
    "                    nn=''\n",
    "            listout.append(nn)\n",
    "    return(listout)\n",
    "    \n",
    "example_nouns = pullnouns(DescList['CCB'][Counts['CCB']['Unq_Donors'][20]]['AllText']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull descriptive words for all donors in our training set (groups)\n",
    "allnouns_pr=[]\n",
    "for j,p in enumerate(groups):\n",
    "    p_banks=[]\n",
    "    p_ids=[]\n",
    "    for i in range(len(p)):\n",
    "        p_bank = p[i][0].strip()\n",
    "        p_id = str(p[i][1])\n",
    "        if bf.find(Counts[p_bank]['Unq_Donors'],p_id): \n",
    "            temp_nouns=stemfn(pullnouns(DescList[p_bank][p_id]['AllText']))\n",
    "            allnouns_pr.extend(temp_nouns)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count how many times each noun appears in the training set donors\n",
    "noun_pr_dict={}\n",
    "for noun in set(allnouns_pr):\n",
    "    noun_pr_dict[noun]=allnouns_pr.count(noun)\n",
    "del(noun_pr_dict[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# identify words that appear more than once in training set\n",
    "noun_list_pr=[]\n",
    "noun_cnt_list_pr=[]\n",
    "for key in noun_pr_dict:\n",
    "    if noun_pr_dict[key]>1:\n",
    "        noun_list_pr.append(key)\n",
    "        noun_cnt_list_pr.append(noun_pr_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words that are incomplete, not really words, previously included as features\n",
    "exclude_list=['oct1983', 'br', 'hey', 'eau', 'd5770', 'cb353', 'mo', 'ame', 'nolan', 'ana',\n",
    "             'msee', 'prior', 'are', 'we', 'prepar', 'ia', 'see', 'help', 'name', 's', 'day', 'lo',\n",
    "             'wi', 'lab', 'left', 'list', 'attend', 'ger', '%', 'boy', 'sperm', 'sens', 'bank',\n",
    "             'new', 'daughter', 'son', 'm.', 'life', 'profil', 'half', 'like', 'speak', 'b', 'ha',\n",
    "             'b.','favorit', 'a', 'mother', 'father', 'o', 'lb', 'type', 'hair', 'ha', 'here', \n",
    "             'ltd', 'born', 'repro', 'same', 'you', 'cli', 'girl', 'love', 'child', 'yr', 'age',\n",
    "             'blue', 'hazel', 'green', 'brown', 'aa', 'latino', 'jewish', ]\n",
    "\n",
    "# eliminated from round zero feature elimination\n",
    "recurs_list=['stock','california','person','pacif','time','fall','sister','servic',\n",
    "           'messag','donat','fine','career','reproduct','health','school','alot', 'stop', \n",
    "           'describ','donor']\n",
    "\n",
    "# eliminate from round 1 feature elimination\n",
    "elim_dict=['mi','univ','medium','nd','mn','attorney','boston','updat','busi','dog','md',\n",
    "           'ma','harvard','bs','a']\n",
    "\n",
    "#eliminate all months (can reference many different things)\n",
    "elim_dict2 = ['complexion', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', \n",
    "              'oct', 'nov', 'dec']\n",
    "\n",
    "include_list=[n for n in noun_list_pr if n not in exclude_list + recurs_list + elim_dict + elim_dict2]\n",
    "\n",
    "len(include_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make dataframe with wordcount\n",
    "noun_df_pr=pd.DataFrame({'noun': noun_list_pr, 'noun_cnt': noun_cnt_list_pr})\n",
    "\n",
    "# Dataframe of included words with counts over donors in the training set, used to examine eliminations\n",
    "# noun_restr=noun_df_pr[noun_df_pr['noun'].isin(include_list)].sort_values(by='noun_cnt').query('noun_cnt == 5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a vector for each donor entering 0 or 1 for every word in include list\n",
    "# input list of strings (from DescList[b][d]['AllText'])\n",
    "def make_noun_vect(text):\n",
    "    temp_nouns = stemfn(pullnouns(DescList[b][d]['AllText']))\n",
    "    #print(temp_nouns)\n",
    "    return [1 if word in temp_nouns else 0 for word in include_list]\n",
    "\n",
    "# make binary matrix of word existence for all donors in database\n",
    "words_alldons=[];\n",
    "for b in banklist:\n",
    "    for d in Counts[b]['Unq_Donors']:\n",
    "        temp_vect = make_noun_vect(DescList[b][d]['AllText'])\n",
    "        words_alldons.append(temp_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add this word information to dataframe of physical and other descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to take a dataframe and a list of columns that need to be \n",
    "# split into separate binary columns\n",
    "# Returns a 3-tuple comprising the data, the vectorized data,\n",
    "# and the fitted vectorizor, adapted from https://gist.github.com/kljensen/5452382\n",
    "def one_hot_dataframe(data, cols, replace=False):\n",
    "    vec = DictVectorizer()\n",
    "    mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
    "    vecData = pd.DataFrame(vec.fit_transform(data[cols].apply(mkdict, axis=1)).toarray())\n",
    "    vecData.columns = vec.get_feature_names()\n",
    "    vecData.index = data.index\n",
    "    if replace is True:\n",
    "        data = data.drop(cols, axis=1)\n",
    "        data = data.join(vecData)\n",
    "    return (data, vecData, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bankid', 'donorid', 'offspcnt', 'offspyr', 'super', 'AA',\n",
       "       'AllText', 'Blue', 'Brown', 'EyeExist', 'Green', 'Hazel', 'Jewish',\n",
       "       'Latino', 'Weight', 'A+', 'A-', 'AB+', 'AB-', 'B+', 'B-', 'O+',\n",
       "       'O-', 'graduat', 'irish', 'clarinet', 'teacher', 'food', 'sibl',\n",
       "       'caucas', 'softwar', 'writer', 'master', 'other', 'photographi',\n",
       "       'public', 'sure', 'studi', 'farsi', 'portland', 'danish', 'post',\n",
       "       'hotel', 'interest', 'cornel', 'christian', 'foot', 'justic',\n",
       "       'retir', 'choctaw', 'american', 'persian', 'cancer', 'astrophys',\n",
       "       'histori', 'read', 'berkeley', 'spanish', 'colleg', 'protest',\n",
       "       'number', 'south', 'tenni', 'white', 'blond', 'wavi', 'educ',\n",
       "       'chiropractor', 'univers', 'famili', 'art', 'restrict', 'wonder',\n",
       "       'canada', 'assist', 'march', 'michigan', 'indian', 'curli',\n",
       "       'biolog', 'lithuanian', 'chemistri', 'programm', 'account',\n",
       "       'russian', 'market', 'music', 'comput', 'relat', 'string', 'dark',\n",
       "       'supervisor', 'write', 'electr', 'research', 'straight', 'parent',\n",
       "       'activ', 'professor', 'tutor', 'brother', 'light', 'german',\n",
       "       'comet', 'militari', 'cathol', 'clerk', 'site', 'scienc', 'degre',\n",
       "       'ms', 'swiss', 'philosophi', 'dutch', 'citi', 'chiropract',\n",
       "       'pleasant', 'languag', 'older', 'econom', 'guitar', 'japanes',\n",
       "       'scottish', 'year', 'medic', 'dc', 'english', 'classic', 'swedish',\n",
       "       'engin', 'polit', 'religion', 'major', 'nice', 'piano', 'avail',\n",
       "       'italian', 'fair', 'norwegian', 'children', 'cuban', 'golf', 'tax',\n",
       "       'more', 'birth', 'psychologist', 'origin', 'student', 'injuri',\n",
       "       'caucasian', 'restaur', 'mba', 'dimpl', 'phd', 'high', 'roman',\n",
       "       'polish', 'chines', 'movi', 'french', 'ba', 'softbal', 'methodist',\n",
       "       'manag', 'law', 'grad', 'crimin', 'bachelor', 'administr',\n",
       "       'musician', 'latin', 'train', 'anim', 'wordcount'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe of word inclusion\n",
    "# add column indicating word count\n",
    "words_alldons_arr=np.array(words_alldons)\n",
    "df_nouns_alldons=pd.DataFrame(data=words_alldons_arr, columns=include_list)\n",
    "df_nouns_alldons['wordcount']=df_nouns_alldons.sum(axis=1)\n",
    "\n",
    "# split out dataframe columns for bloodtype into separate categorical indicators\n",
    "# make convert column names blood type only for more ease in SQL\n",
    "bloodencode_df = one_hot_dataframe(donor_df, ['BloodType'], replace=True)\n",
    "for c in bloodencode_df[1].columns.values:\n",
    "    if 'BloodType=' in c:\n",
    "        cnew=c[10:]\n",
    "        bloodencode_df[0][cnew]=bloodencode_df[0][c]\n",
    "\n",
    "donor_df_new=bloodencode_df[0]\n",
    "for c in donor_df_new.columns.values:\n",
    "    if 'BloodType' in c:\n",
    "        donor_df_new=donor_df_new.drop(c,1)\n",
    "\n",
    "# combine dataframe of word existence with old dataframe that has categorical info per donor\n",
    "df_donor_withwords=pd.concat([donor_df_new, df_nouns_alldons],axis=1)\n",
    "\n",
    "# replace year nans with mean offspring birth year\n",
    "df_donor_withwords['offspyr'].fillna(np.nanmean(df_donor_withwords['offspyr'].tolist()), inplace=True)\n",
    "\n",
    "# replace weight nans with mean weight\n",
    "df_donor_withwords['Weight'].fillna(np.nanmean(df_donor_withwords['Weight'].tolist()), inplace=True)\n",
    "\n",
    "# look at columns and make sure they seem OK\n",
    "df_donor_withwords.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save database that includes parsed features from text and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189 Thereareatotalof75offspring,59siblingmatches,and0donortooffspringmatchesfromthisClinic.\n",
      "3267 Thereareatotalof6offspring,0siblingmatches,and0donortooffspringmatchesfromthisClinic.\n"
     ]
    }
   ],
   "source": [
    "# Cull donor IDs that are errors\n",
    "badids=[]\n",
    "for i, k in enumerate(df_donor_withwords['donorid']):\n",
    "    if len(k)>50:\n",
    "        print(i, k)\n",
    "        badids.append(i)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bankid</th>\n",
       "      <th>donorid</th>\n",
       "      <th>offspcnt</th>\n",
       "      <th>offspyr</th>\n",
       "      <th>super</th>\n",
       "      <th>AA</th>\n",
       "      <th>AllText</th>\n",
       "      <th>Blue</th>\n",
       "      <th>Brown</th>\n",
       "      <th>EyeExist</th>\n",
       "      <th>...</th>\n",
       "      <th>law</th>\n",
       "      <th>grad</th>\n",
       "      <th>crimin</th>\n",
       "      <th>bachelor</th>\n",
       "      <th>administr</th>\n",
       "      <th>musician</th>\n",
       "      <th>latin</th>\n",
       "      <th>train</th>\n",
       "      <th>anim</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>IBBER</td>\n",
       "      <td>4</td>\n",
       "      <td>2012.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>2677</td>\n",
       "      <td>1</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>LANCE</td>\n",
       "      <td>13</td>\n",
       "      <td>2011.923077</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>2771FINN</td>\n",
       "      <td>2</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>3836</td>\n",
       "      <td>1</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>TOPPER</td>\n",
       "      <td>1</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>6533</td>\n",
       "      <td>2</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>6095KIRK</td>\n",
       "      <td>1</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>UGGER</td>\n",
       "      <td>1</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>5586</td>\n",
       "      <td>2</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>5524</td>\n",
       "      <td>1</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>688DK</td>\n",
       "      <td>1</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>3584ADAM</td>\n",
       "      <td>1</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>8116WILAS</td>\n",
       "      <td>1</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>GEIR</td>\n",
       "      <td>4</td>\n",
       "      <td>2005.750000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>3377BENT</td>\n",
       "      <td>2</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>782TIMM</td>\n",
       "      <td>4</td>\n",
       "      <td>2011.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>3088KNUT</td>\n",
       "      <td>1</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>1290</td>\n",
       "      <td>1</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>5875</td>\n",
       "      <td>1</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>7229NEVIL</td>\n",
       "      <td>1</td>\n",
       "      <td>2002.023452</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>2453</td>\n",
       "      <td>1</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>5597</td>\n",
       "      <td>1</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>3107LARS</td>\n",
       "      <td>6</td>\n",
       "      <td>2007.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>8521GIZMO</td>\n",
       "      <td>1</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>6196</td>\n",
       "      <td>1</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>2092</td>\n",
       "      <td>2</td>\n",
       "      <td>1999.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>DARIN</td>\n",
       "      <td>1</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>7110STEEN</td>\n",
       "      <td>1</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cryos</td>\n",
       "      <td>3704DAGH</td>\n",
       "      <td>12</td>\n",
       "      <td>2009.583333</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1985,AUG.</td>\n",
       "      <td>1</td>\n",
       "      <td>1986.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>MLH</td>\n",
       "      <td>1</td>\n",
       "      <td>1995.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1981,SEPT.</td>\n",
       "      <td>1</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1974</td>\n",
       "      <td>1</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1979,FEB.</td>\n",
       "      <td>1</td>\n",
       "      <td>1979.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1988,DEC.</td>\n",
       "      <td>1</td>\n",
       "      <td>1989.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1978-1979SCHOOLYEAR</td>\n",
       "      <td>1</td>\n",
       "      <td>1979.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>EASTBAYMATCH#1</td>\n",
       "      <td>1</td>\n",
       "      <td>1981.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>MLP</td>\n",
       "      <td>1</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1980,JULY</td>\n",
       "      <td>1</td>\n",
       "      <td>1981.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1979</td>\n",
       "      <td>1</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>C.1977,1979</td>\n",
       "      <td>2</td>\n",
       "      <td>1979.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3302</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>EASTBAYMATCH#2</td>\n",
       "      <td>2</td>\n",
       "      <td>1985.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>EBMC</td>\n",
       "      <td>JTB</td>\n",
       "      <td>1</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3304</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N267</td>\n",
       "      <td>2</td>\n",
       "      <td>1998.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N238</td>\n",
       "      <td>1</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N214</td>\n",
       "      <td>3</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>RMC</td>\n",
       "      <td>S113</td>\n",
       "      <td>1</td>\n",
       "      <td>1987.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>RMC</td>\n",
       "      <td>B158</td>\n",
       "      <td>2</td>\n",
       "      <td>2000.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N140</td>\n",
       "      <td>2</td>\n",
       "      <td>1991.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N153</td>\n",
       "      <td>2</td>\n",
       "      <td>1997.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N223</td>\n",
       "      <td>1</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>RMC</td>\n",
       "      <td>B132</td>\n",
       "      <td>4</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N127</td>\n",
       "      <td>3</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>RMC</td>\n",
       "      <td>B125</td>\n",
       "      <td>1</td>\n",
       "      <td>1989.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N170</td>\n",
       "      <td>3</td>\n",
       "      <td>2006.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>RMC</td>\n",
       "      <td>N244</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>RMC</td>\n",
       "      <td>B120</td>\n",
       "      <td>5</td>\n",
       "      <td>2000.600000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>RMC</td>\n",
       "      <td>B115</td>\n",
       "      <td>1</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>RMC</td>\n",
       "      <td>B171</td>\n",
       "      <td>2</td>\n",
       "      <td>2002.023452</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3318 rows × 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bankid                donorid  offspcnt      offspyr  super  AA  AllText  \\\n",
       "0     Cryos                  IBBER         4  2012.250000  False   0        1   \n",
       "1     Cryos                   2677         1  2004.000000  False   0        1   \n",
       "2     Cryos                  LANCE        13  2011.923077   True   0        1   \n",
       "3     Cryos               2771FINN         2  2008.000000  False   0        0   \n",
       "4     Cryos                   3836         1  2009.000000  False   0        1   \n",
       "5     Cryos                 TOPPER         1  2015.000000  False   0        0   \n",
       "6     Cryos                   6533         2  2012.000000  False   0        1   \n",
       "7     Cryos               6095KIRK         1  2011.000000  False   0        1   \n",
       "8     Cryos                  UGGER         1  2015.000000  False   0        0   \n",
       "9     Cryos                   5586         2  2010.000000  False   0        1   \n",
       "10    Cryos                   5524         1  2008.000000  False   0        1   \n",
       "11    Cryos                  688DK         1  2004.000000  False   0        1   \n",
       "12    Cryos               3584ADAM         1  2005.000000  False   0        1   \n",
       "13    Cryos              8116WILAS         1  2012.000000  False   0        1   \n",
       "14    Cryos                   GEIR         4  2005.750000  False   0        1   \n",
       "15    Cryos               3377BENT         2  2006.000000  False   0        1   \n",
       "16    Cryos                782TIMM         4  2011.250000  False   0        1   \n",
       "17    Cryos               3088KNUT         1  2007.000000  False   0        1   \n",
       "18    Cryos                   1290         1  2006.000000  False   0        1   \n",
       "19    Cryos                   5875         1  2011.000000  False   0        0   \n",
       "20    Cryos              7229NEVIL         1  2002.023452  False   0        0   \n",
       "21    Cryos                   2453         1  2003.000000  False   0        1   \n",
       "22    Cryos                   5597         1  2010.000000  False   0        0   \n",
       "23    Cryos               3107LARS         6  2007.500000   True   0        1   \n",
       "24    Cryos              8521GIZMO         1  2015.000000  False   0        1   \n",
       "25    Cryos                   6196         1  2010.000000  False   0        1   \n",
       "26    Cryos                   2092         2  1999.500000  False   0        1   \n",
       "27    Cryos                  DARIN         1  2015.000000  False   0        1   \n",
       "28    Cryos              7110STEEN         1  2009.000000  False   0        1   \n",
       "29    Cryos               3704DAGH        12  2009.583333   True   0        1   \n",
       "...     ...                    ...       ...          ...    ...  ..      ...   \n",
       "3290   EBMC            C.1985,AUG.         1  1986.000000  False   0        1   \n",
       "3291   EBMC                    MLH         1  1995.000000  False   0        1   \n",
       "3292   EBMC           C.1981,SEPT.         1  1982.000000  False   0        1   \n",
       "3293   EBMC                 C.1974         1  1974.000000  False   0        1   \n",
       "3294   EBMC            C.1979,FEB.         1  1979.000000  False   0        1   \n",
       "3295   EBMC            C.1988,DEC.         1  1989.000000  False   1        1   \n",
       "3296   EBMC  C.1978-1979SCHOOLYEAR         1  1979.000000  False   0        1   \n",
       "3297   EBMC         EASTBAYMATCH#1         1  1981.000000  False   0        0   \n",
       "3298   EBMC                    MLP         1  1993.000000  False   0        1   \n",
       "3299   EBMC            C.1980,JULY         1  1981.000000  False   0        1   \n",
       "3300   EBMC                 C.1979         1  1980.000000  False   0        1   \n",
       "3301   EBMC            C.1977,1979         2  1979.000000  False   0        1   \n",
       "3302   EBMC         EASTBAYMATCH#2         2  1985.500000  False   0        1   \n",
       "3303   EBMC                    JTB         1  1998.000000  False   0        1   \n",
       "3304    RMC                   N267         2  1998.500000  False   0        1   \n",
       "3305    RMC                   N238         1  1993.000000  False   0        1   \n",
       "3306    RMC                   N214         3  1993.000000  False   0        1   \n",
       "3307    RMC                   S113         1  1987.000000  False   0        1   \n",
       "3308    RMC                   B158         2  2000.500000  False   0        1   \n",
       "3309    RMC                   N140         2  1991.500000  False   0        1   \n",
       "3310    RMC                   N153         2  1997.500000  False   0        1   \n",
       "3311    RMC                   N223         1  1993.000000  False   0        1   \n",
       "3312    RMC                   B132         4  1993.000000  False   0        1   \n",
       "3313    RMC                   N127         3  2000.000000  False   0        1   \n",
       "3314    RMC                   B125         1  1989.000000  False   0        1   \n",
       "3315    RMC                   N170         3  2006.333333  False   0        1   \n",
       "3316    RMC                   N244         1  2000.000000  False   0        1   \n",
       "3317    RMC                   B120         5  2000.600000  False   0        1   \n",
       "3318    RMC                   B115         1  2005.000000  False   0        1   \n",
       "3319    RMC                   B171         2  2002.023452  False   0        1   \n",
       "\n",
       "      Blue  Brown  EyeExist    ...      law  grad  crimin  bachelor  \\\n",
       "0        1      0         1    ...        0     0       0         1   \n",
       "1        1      0         1    ...        0     0       0         0   \n",
       "2        1      0         1    ...        0     0       0         0   \n",
       "3        0      0         0    ...        0     0       0         0   \n",
       "4        1      0         1    ...        0     0       0         0   \n",
       "5        0      0         0    ...        0     0       0         0   \n",
       "6        1      0         1    ...        0     0       0         0   \n",
       "7        0      1         1    ...        0     0       0         1   \n",
       "8        0      0         0    ...        0     0       0         0   \n",
       "9        0      0         1    ...        0     0       0         0   \n",
       "10       1      0         1    ...        0     0       0         1   \n",
       "11       1      0         1    ...        0     0       0         0   \n",
       "12       1      0         1    ...        0     0       0         0   \n",
       "13       0      0         1    ...        0     0       0         0   \n",
       "14       1      0         1    ...        0     0       0         0   \n",
       "15       1      0         1    ...        0     0       0         0   \n",
       "16       0      1         1    ...        0     0       0         0   \n",
       "17       1      0         1    ...        0     0       0         0   \n",
       "18       0      1         1    ...        0     0       0         0   \n",
       "19       0      0         0    ...        0     0       0         0   \n",
       "20       0      0         0    ...        0     0       0         0   \n",
       "21       1      0         1    ...        0     0       0         0   \n",
       "22       0      0         0    ...        0     0       0         0   \n",
       "23       1      0         1    ...        0     0       0         0   \n",
       "24       0      0         0    ...        0     0       0         0   \n",
       "25       1      0         1    ...        0     0       0         0   \n",
       "26       1      0         1    ...        0     0       0         0   \n",
       "27       0      0         1    ...        0     0       0         0   \n",
       "28       0      0         1    ...        0     0       0         0   \n",
       "29       1      0         1    ...        0     0       0         0   \n",
       "...    ...    ...       ...    ...      ...   ...     ...       ...   \n",
       "3290     0      0         0    ...        0     0       0         0   \n",
       "3291     0      0         1    ...        0     0       0         0   \n",
       "3292     0      1         1    ...        0     0       0         0   \n",
       "3293     1      1         1    ...        0     0       0         0   \n",
       "3294     0      0         0    ...        0     0       0         0   \n",
       "3295     0      0         1    ...        0     0       0         0   \n",
       "3296     0      1         1    ...        0     0       0         0   \n",
       "3297     0      0         0    ...        0     0       0         0   \n",
       "3298     0      1         1    ...        0     0       0         0   \n",
       "3299     1      0         1    ...        0     0       0         0   \n",
       "3300     1      0         1    ...        0     0       0         0   \n",
       "3301     1      0         1    ...        0     0       0         0   \n",
       "3302     0      1         1    ...        0     0       0         0   \n",
       "3303     1      0         1    ...        0     0       0         0   \n",
       "3304     0      1         1    ...        0     0       0         0   \n",
       "3305     0      1         1    ...        0     0       0         0   \n",
       "3306     0      1         1    ...        0     0       0         0   \n",
       "3307     1      0         1    ...        0     0       0         0   \n",
       "3308     1      0         1    ...        0     0       0         0   \n",
       "3309     1      0         1    ...        0     0       0         0   \n",
       "3310     1      0         1    ...        0     0       0         0   \n",
       "3311     0      1         1    ...        0     0       0         0   \n",
       "3312     1      0         1    ...        0     0       0         0   \n",
       "3313     1      0         1    ...        0     0       0         0   \n",
       "3314     1      0         1    ...        0     0       0         0   \n",
       "3315     1      0         1    ...        1     0       1         0   \n",
       "3316     0      1         1    ...        0     0       0         0   \n",
       "3317     1      0         1    ...        0     0       0         0   \n",
       "3318     1      0         1    ...        0     0       0         0   \n",
       "3319     0      0         0    ...        0     0       0         0   \n",
       "\n",
       "      administr  musician  latin  train  anim  wordcount  \n",
       "0             0         0      0      0     0          8  \n",
       "1             0         0      0      0     0          1  \n",
       "2             0         0      0      0     0          8  \n",
       "3             0         0      0      0     0          0  \n",
       "4             0         0      0      0     0          3  \n",
       "5             0         0      0      0     0          0  \n",
       "6             0         0      0      0     0          2  \n",
       "7             0         0      0      0     0          7  \n",
       "8             0         0      0      0     0          0  \n",
       "9             0         0      0      0     0          2  \n",
       "10            0         0      0      0     0          3  \n",
       "11            0         0      0      0     0          3  \n",
       "12            0         0      0      0     0          4  \n",
       "13            0         0      0      0     0          1  \n",
       "14            0         0      0      1     0          5  \n",
       "15            0         0      0      0     0          1  \n",
       "16            0         0      0      0     0         11  \n",
       "17            0         0      0      0     0          2  \n",
       "18            0         0      0      0     0          5  \n",
       "19            0         0      0      0     0          0  \n",
       "20            0         0      0      0     0          0  \n",
       "21            0         0      0      0     0          3  \n",
       "22            0         0      0      0     0          0  \n",
       "23            0         0      0      0     0          5  \n",
       "24            0         0      0      0     0          2  \n",
       "25            0         0      0      0     0          4  \n",
       "26            0         0      0      0     0          2  \n",
       "27            0         0      0      0     0          7  \n",
       "28            0         0      0      0     0          2  \n",
       "29            0         0      0      0     0          6  \n",
       "...         ...       ...    ...    ...   ...        ...  \n",
       "3290          0         0      0      0     0          1  \n",
       "3291          0         0      0      0     0          5  \n",
       "3292          0         0      0      0     0          1  \n",
       "3293          0         0      0      0     0          3  \n",
       "3294          0         0      0      0     0          0  \n",
       "3295          0         0      0      0     0          5  \n",
       "3296          0         0      0      0     0          4  \n",
       "3297          0         0      0      0     0          0  \n",
       "3298          0         0      0      0     0          8  \n",
       "3299          0         0      0      0     0          2  \n",
       "3300          0         0      0      0     0          4  \n",
       "3301          0         0      0      0     0          4  \n",
       "3302          0         0      0      0     0          3  \n",
       "3303          0         0      0      0     0          6  \n",
       "3304          0         0      0      0     0          2  \n",
       "3305          0         0      0      0     0          4  \n",
       "3306          0         0      0      0     0          6  \n",
       "3307          0         0      0      0     0          3  \n",
       "3308          0         0      0      0     0          3  \n",
       "3309          0         0      0      0     0          5  \n",
       "3310          0         0      0      0     0          3  \n",
       "3311          0         0      0      0     0          2  \n",
       "3312          0         0      0      0     0          5  \n",
       "3313          0         0      0      0     0          9  \n",
       "3314          0         0      0      0     0          3  \n",
       "3315          0         0      0      0     0         16  \n",
       "3316          0         0      0      0     0          5  \n",
       "3317          0         0      0      0     0         11  \n",
       "3318          0         0      0      0     0          5  \n",
       "3319          0         0      0      0     0          3  \n",
       "\n",
       "[3318 rows x 174 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut bad donor ids from database before saving\n",
    "df_donor_withwords.drop(df_donor_withwords.index[badids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://nathanvc:5698@localhost/dsr_db6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Make Database\n",
    "dbname2 = 'dsr_db6'\n",
    "username = 'nathanvc'\n",
    "pswd = '5698'\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname2))\n",
    "print(engine.url)\n",
    "# Replace localhost with IP address if accessing a remote server\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "   create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_donor_withwords.columns = map(str.lower, df_donor_withwords.columns)\n",
    "df_donor_withwords.head\n",
    "df_donor_withwords.to_sql(dbname2, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LMNN with physical features and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local database connection\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname2, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load information for donors from training set from database\n",
    "# add column indicating group membership (used for training LMNN metric)\n",
    "# Note that field names require a little fiddling\n",
    "for i in range(len(groups)):\n",
    "    \n",
    "    donorid1=str(groups[i][0][1])\n",
    "    donorid2=str(groups[i][1][1])\n",
    "    bankid1=str(groups[i][0][0])\n",
    "    bankid2=str(groups[i][1][0])\n",
    "    \n",
    "    if len(groups[i])==3:\n",
    "        donorid3=str(groups[i][2][1])\n",
    "        bankid3=str(groups[i][2][0])\n",
    "      \n",
    "    if donorid2 =='236.0':\n",
    "        donorid2='236'\n",
    "    if donorid2 =='5027.0':\n",
    "        donorid2='5027'\n",
    "    if donorid2 =='3267.0':\n",
    "        donorid2='3267'\n",
    "    if donorid2 =='270.0':\n",
    "        donorid2='270'\n",
    "    if bankid2=='Paces ':\n",
    "        bankid2='Paces'\n",
    "    if bankid2=='Zygen ':\n",
    "        bankid2='Zygen'\n",
    "        \n",
    "    if donorid3:\n",
    "        if donorid3 =='236.0':\n",
    "            donorid3='236'\n",
    "        if donorid3 =='5027.0':\n",
    "            donorid3='5027'\n",
    "        if donorid3 =='3267.0':\n",
    "            donorid3='3267'\n",
    "        if donorid3 =='270.0':\n",
    "            donorid3='270'\n",
    "    if bankid3:\n",
    "        if bankid3=='Paces ':\n",
    "            bankid3='Paces'\n",
    "        if bankid3=='Zygen ':\n",
    "            bankid3='Zygen'\n",
    " \n",
    "    if len(groups[i])==2:\n",
    "        query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM dsr_db6 \n",
    "        WHERE (bankid = '%s' AND donorid = '%s')\n",
    "        OR (bankid = '%s' AND donorid = '%s')\n",
    "        \"\"\" % (bankid1, donorid1, bankid2, donorid2)\n",
    "        \n",
    "\n",
    "    elif len(groups[i])==3:\n",
    "        query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM dsr_db6 \n",
    "        WHERE (bankid = '%s' AND donorid = '%s')\n",
    "        OR (bankid = '%s' AND donorid = '%s')\n",
    "        OR (bankid = '%s' AND donorid = '%s')\n",
    "        \"\"\" % (bankid1, donorid1, bankid2, donorid2, bankid3, donorid3)\n",
    "    \n",
    "    # Add group membership indicator, one index per group starting from 0\n",
    "    if i == 0:\n",
    "        prs_w = pd.read_sql_query(query,con)\n",
    "        prs_w['pairid']=[0]*len(prs_w)\n",
    "    else:\n",
    "        pr_indiv = pd.read_sql_query(query, con)\n",
    "        pr_indiv['pairid']=[i]*len(pr_indiv)\n",
    "        prs_w = pd.concat([prs_w, pr_indiv], axis=0)\n",
    "        \n",
    "# drop columns that we don't need for this problem\n",
    "# prs_x is our features for training\n",
    "prs_x=prs_w\n",
    "prs_x = prs_x.drop('index', 1)\n",
    "prs_x = prs_x.drop('offspcnt', 1)\n",
    "prs_x = prs_x.drop('super', 1)\n",
    "prs_x = prs_x.drop('alltext', 1)\n",
    "prs_x = prs_x.drop('bankid', 1)\n",
    "prs_x = prs_x.drop('donorid', 1)\n",
    "prs_x = prs_x.drop('pairid', 1)\n",
    "prs_x = prs_x.drop('eyeexist', 1)\n",
    "prs_x = prs_x.drop('wordcount', 1)\n",
    "\n",
    "# group membership information that we are training our LMNN metric\n",
    "prs_y = prs_w['pairid']\n",
    "\n",
    "# donor info, will use this to recapture donor information with the same indexing\n",
    "prs_dinfo = pd.concat([prs_w['bankid'], prs_w['donorid']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe version and array version of training data features\n",
    "prs_x_df=prs_x\n",
    "prs_x=np.array(prs_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for cross-validation and running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to run leave one out cross-validation\n",
    "# Calculates LMNN metric on training data with one class removed\n",
    "# Uses this metric to calculate distance from all members of training set\n",
    "# for class that was left out\n",
    "# Outputs dataframe of these distances in dist_df\n",
    "# W_all is a list of the metrics for each training of the model\n",
    "def run_crossval_LMNN(prs_x, prs_y, prs_dinfo):\n",
    "    \n",
    "    labelnum=0;\n",
    "    W_all=[]\n",
    "    \n",
    "    # make dictionary indexed by only pair IDs that has bankid and donor info\n",
    "    dinfo = np.array(prs_dinfo)\n",
    "    dinfo_dict={}\n",
    "    dinfo_dict['bankid']=dinfo[:,0]\n",
    "    dinfo_dict['donorid']=dinfo[:,1]\n",
    "    dinfo_dict['pairid']=list(prs_y)\n",
    "    dist_df=pd.DataFrame.from_dict(dinfo_dict)\n",
    "    \n",
    "    dist_dict={}\n",
    "\n",
    "    for d in set(prs_y):\n",
    "        leaveout = bf.find(prs_y,d)\n",
    "        keep = [i for i in range(len(prs_y)) if i not in leaveout]\n",
    "    \n",
    "        prs_temp=prs_x[keep,:]\n",
    "        y_temp = np.array(prs_y)[keep]\n",
    "    \n",
    "        bank_out = dinfo[leaveout,0]\n",
    "        id_out = dinfo[leaveout,1]\n",
    "    \n",
    "        bank_in = dinfo[keep,0]\n",
    "        id_in = dinfo[keep,1]\n",
    "    \n",
    "        lmnn_prs_cv = LMNN(k=2)\n",
    "        lmnn_prs_cv.fit(prs_temp, y_temp, verbose=False)\n",
    "        W_cv=lmnn_prs_cv.metric()\n",
    "        \n",
    "        dist_dict={}\n",
    "        for k in leaveout:\n",
    "            dist_cv=[]\n",
    "            label='dist'+str(labelnum)\n",
    "            for a in range(len(prs_y)):\n",
    "                dist=prs_x[k,:]-prs_x[a,:]         \n",
    "                #distance adjusted by metric learning\n",
    "                lmnn_dist=np.sqrt(np.dot(dist.T,np.dot(W_cv,dist)))\n",
    "                dist_cv.append(lmnn_dist)\n",
    "            dist_dict[label]=dist_cv\n",
    "            labelnum=labelnum+1\n",
    "        pd_dist=pd.DataFrame.from_dict(dist_dict)\n",
    "        dist_df=pd.concat([dist_df,pd_dist],axis=1)\n",
    "        \n",
    "        W_all.append(W_cv)\n",
    "\n",
    "    return (dist_df, W_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to run model on full training set\n",
    "# (this is what goes into the model at super-donor)\n",
    "# Returns only the LMNN metric W\n",
    "def run_all_LMNN(prs_x, prs_y, prs_dinfo):    \n",
    "    lmnn_prs = LMNN(k=2)\n",
    "    lmnn_prs.fit(prs_x, prs_y, verbose=False)\n",
    "    W=lmnn_prs.metric()\n",
    "    return (W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to pull all entries in either the training set or the database\n",
    "# sorted by distance from the specified donor in the training set\n",
    "def pullall(dist_df, d):\n",
    "    label='dist'+str(d)\n",
    "    return np.array(dist_df[['donorid','bankid','pairid',label]].sort_values(by=label))\n",
    "\n",
    "def pullalldb(dist_df, d):\n",
    "    label='dist'+str(d)\n",
    "    return np.array(dist_df[['bankid','donorid',label]].sort_values(by=label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cross-validation, calculate distance from all donors in database for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run cross validation\n",
    "dist_df, W_all = run_crossval_LMNN(prs_x, prs_y, prs_dinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import and make array of info for every single donor in registry\n",
    "con = psycopg2.connect(database = dbname2, user = username, host='localhost', password=pswd)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM dsr_db6 \n",
    "\"\"\"\n",
    "prs_all = pd.read_sql_query(query,con)\n",
    "  \n",
    "prs_a=prs_all\n",
    "prs_a = prs_a.drop('index', 1)\n",
    "prs_a = prs_a.drop('offspcnt', 1)\n",
    "prs_a = prs_a.drop('super', 1)\n",
    "prs_a = prs_a.drop('alltext', 1)\n",
    "prs_a = prs_a.drop('bankid', 1)\n",
    "prs_a = prs_a.drop('donorid', 1)\n",
    "prs_a = prs_a.drop('eyeexist', 1)\n",
    "prs_a = prs_a.drop('wordcount', 1)\n",
    "\n",
    "# donor info\n",
    "prs_adinfo = pd.concat([prs_all['bankid'], prs_all['donorid']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate distance from all donors in the database\n",
    "# for donors in the training set\n",
    "# Uses cross validation distance\n",
    "def calc_distdb_crossval(prs_x, prs_y, prs_a, W_all):\n",
    "    \n",
    "    labelnum=0;\n",
    "    \n",
    "    # make dictionary indexed by only pair IDs that has bankid and donor info\n",
    "    dinfo = np.array(prs_adinfo)\n",
    "    dinfo_dict={}\n",
    "    dinfo_dict['bankid']=dinfo[:,0]\n",
    "    dinfo_dict['donorid']=dinfo[:,1]\n",
    "    #dinfo_dict['pairid']=list(prs_y)\n",
    "    dist_df=pd.DataFrame.from_dict(dinfo_dict)\n",
    "    \n",
    "    dist_dict={}\n",
    "    \n",
    "    prs_a=np.array(prs_a)\n",
    "    for d in set(prs_y):\n",
    "        leaveout = bf.find(prs_y,d)\n",
    "        keep = [i for i in range(len(prs_y)) if i not in leaveout]\n",
    "    \n",
    "        dist_dict={}\n",
    "        for k in leaveout:\n",
    "            dist_cv=[]\n",
    "            label='dist'+str(labelnum)\n",
    "            for a in range(len(prs_a)):\n",
    "                dist=prs_x[k,:]-prs_a[a,:]         \n",
    "                #distance adjusted by metric learning\n",
    "                lmnn_dist=np.sqrt(np.dot(dist.T,np.dot(W_all[d],dist)))\n",
    "                dist_cv.append(lmnn_dist)\n",
    "                #print(a, k, label)\n",
    "            dist_dict[label]=dist_cv\n",
    "            labelnum=labelnum+1\n",
    "        pd_dist=pd.DataFrame.from_dict(dist_dict)\n",
    "        dist_df=pd.concat([dist_df,pd_dist],axis=1)\n",
    "    \n",
    "    return(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate cross validation distance from all donors in database\n",
    "# for donors in the training set\n",
    "dist_df_db_cv = calc_distdb_crossval(prs_x, prs_y, prs_a, W_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run model on full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train model on full training set (this is what gets deployed in the web-app at super-donor.com)\n",
    "W = run_all_LMNN(prs_x, prs_y, prs_dinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save file with this matrix (used in the web app)\n",
    "# np.save('LMNN_mat6', W, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distances using cross-validation distance, full LMNN distance, & euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make matrix of distances (cut off donor info)\n",
    "df_dist_cv_mat=np.array(dist_df.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate full model and euclidean distance over full database\n",
    "dist_all=[]\n",
    "eucl_all=[]\n",
    "dist_temp=[]\n",
    "eucl_temp=[]\n",
    "prs_a_df=prs_a\n",
    "prs_a=np.array(prs_a)\n",
    "for i in range(prs_x.shape[0]):\n",
    "    dist_temp=[]\n",
    "    eucl_temp=[]\n",
    "    for d in range(prs_a.shape[0]):\n",
    "        dist=prs_x[i,:]-prs_a[d,:] \n",
    "        lmnn_dist=np.sqrt(np.dot(dist.T,np.dot(W,dist)))\n",
    "        eucl_dist=np.sqrt(np.dot(dist.T,dist))\n",
    "        dist_temp.append(lmnn_dist)\n",
    "        eucl_temp.append(eucl_dist)\n",
    "    dist_all.append(dist_temp)\n",
    "    eucl_all.append(eucl_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distances with cross-validation, euclidean distance, and full-model over \n",
    "# training set only\n",
    "dist_train=[]\n",
    "eucl_train=[]\n",
    "dist_train_cv=[]\n",
    "dist_temp=[]\n",
    "eucl_temp=[]\n",
    "cv_temp=[]\n",
    "prs_a_df=prs_a\n",
    "prs_a=np.array(prs_a)\n",
    "for i in range(prs_x.shape[0]):\n",
    "    dist_temp=[]\n",
    "    eucl_temp=[]\n",
    "    cv_temp=[]\n",
    "    for d in range(prs_x.shape[0]):\n",
    "        dist=prs_x[i,:]-prs_x[d,:] \n",
    "        lmnn_dist=np.sqrt(np.dot(dist.T,np.dot(W,dist)))\n",
    "        eucl_dist=np.sqrt(np.dot(dist.T,dist))\n",
    "        cv_dist=df_dist_cv_mat[d,i]\n",
    "        dist_temp.append(lmnn_dist)\n",
    "        eucl_temp.append(eucl_dist)\n",
    "        cv_temp.append(cv_dist)\n",
    "    dist_train.append(dist_temp)\n",
    "    eucl_train.append(eucl_temp)\n",
    "    dist_train_cv.append(cv_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate and plot distances for pairs and non-pairs in the training set\n",
    "# using cross-validation distance, full model distance and euclidean distance\n",
    "pair_dist_lrn = []\n",
    "nonpair_dist_lrn = []\n",
    "pair_dist_lrn_cv = []\n",
    "nonpair_dist_lrn_cv = []\n",
    "pair_dist_eucl = []\n",
    "nonpair_dist_eucl = []\n",
    "\n",
    "for d in range(prs_x.shape[0]):\n",
    "    cat=np.array(prs_y)[d]\n",
    "    groupind = bf.find(prs_y,cat)\n",
    "    outgroupind = [i for i in range(len(prs_y)) if i not in groupind]\n",
    "    for g in groupind:\n",
    "        if g!=d:\n",
    "            pair_dist_lrn.extend([dist_train[d][g]])\n",
    "            pair_dist_eucl.extend([eucl_train[d][g]])\n",
    "            pair_dist_lrn_cv.extend([dist_train_cv[d][g]])\n",
    "    for g in outgroupind:\n",
    "        if g!=d:\n",
    "            nonpair_dist_lrn.extend([dist_train[d][g]])\n",
    "            nonpair_dist_eucl.extend([eucl_train[d][g]])\n",
    "            nonpair_dist_lrn_cv.extend([dist_train_cv[d][g]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot distances for pairs and non-pairs in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot metric learned distance on full training set\n",
    "plt.figure()\n",
    "binsboth=np.linspace(0,6,num=25)\n",
    "weights_np = np.ones_like(nonpair_dist_lrn)/len(nonpair_dist_lrn)\n",
    "weights_p = np.ones_like(pair_dist_lrn)/len(pair_dist_lrn)\n",
    "plt.hist(pair_dist_lrn, alpha=0.3, color='blue',normed=0,bins=binsboth, weights=weights_p)\n",
    "plt.hist(nonpair_dist_lrn,alpha=0.3, color='green',normed=0,bins=binsboth, weights=weights_np)\n",
    "plt.title('Metric Learning Distance', fontsize=20)\n",
    "plt.xlabel('Distance', fontsize=18)\n",
    "plt.ylabel('Normalized Count', fontsize=18)\n",
    "plt.legend(['Matched Donors', 'Non-Matched Donors'], loc='best', fontsize=15)\n",
    "plt.show()\n",
    "plt.savefig('figures/MetricLearningDistanceHist_noCV_rev.pdf')\n",
    "plt.savefig('figures/MetricLearningDistanceHist_noCV_rev.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot Euclidean Distance on training set\n",
    "plt.figure()\n",
    "binsboth=np.linspace(0,80,num=25)\n",
    "weights_np = np.ones_like(nonpair_dist_eucl)/len(nonpair_dist_eucl)\n",
    "weights_p = np.ones_like(pair_dist_eucl)/len(pair_dist_eucl)\n",
    "plt.hist(pair_dist_eucl,alpha=0.3, color='blue',normed=0, bins=binsboth, weights=weights_p)\n",
    "plt.hist(nonpair_dist_eucl,alpha=0.3, color='green',normed=0, bins=binsboth, weights=weights_np)\n",
    "plt.title('Euclidean Distance', fontsize=20)\n",
    "plt.xlabel('Distance', fontsize=18)\n",
    "plt.ylabel('Normalized Count', fontsize=18)\n",
    "plt.legend(['Matched Donors', 'Non-Matched Donors'], loc='best', fontsize=15)\n",
    "plt.show()\n",
    "plt.savefig('figures/EuclideanDistanceHist_rev.pdf')\n",
    "plt.savefig('figures/EuclideanDistanceHist_rev.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot metric learned distance with cross validation\n",
    "plt.figure()\n",
    "binsboth=np.linspace(0,6,num=25)\n",
    "weights_np = np.ones_like(nonpair_dist_lrn_cv)/len(nonpair_dist_lrn_cv)\n",
    "weights_p = np.ones_like(pair_dist_lrn_cv)/len(pair_dist_lrn_cv)\n",
    "plt.hist(pair_dist_lrn_cv, alpha=0.3, color='blue',normed=0,bins=binsboth, weights=weights_p)\n",
    "plt.hist(nonpair_dist_lrn_cv,alpha=0.3, color='green',normed=0,bins=binsboth, weights=weights_np)\n",
    "plt.title('Metric Learning Distance, Cross-Validation', fontsize=20)\n",
    "plt.xlabel('Distance', fontsize=18)\n",
    "plt.ylabel('Normalized Count', fontsize=18)\n",
    "plt.legend(['Matched Donors', 'Non-Matched Donors'], loc='best', fontsize=15)\n",
    "plt.show()\n",
    "plt.savefig('figures/MetricLearningDistanceHist_CV_rev.pdf')\n",
    "plt.savefig('figures/MetricLearningDistanceHist_CV_rev.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model on the full database\n",
    "def eval_model_fulldb(dist_df, prs_x, prs_dinfo, prs_y):\n",
    "    infoarray=np.array(prs_dinfo)\n",
    "    df_val={}\n",
    "    all_match=[]\n",
    "    match_dist=[]\n",
    "    nonpair_dist=[]\n",
    "    match_dist_form=[]\n",
    "    labelnum=0;\n",
    "    for d in set(prs_y):\n",
    "        groupind = bf.find(prs_y,d)\n",
    "        for k in groupind:\n",
    "            rank_match=[]\n",
    "            label='dist'+str(labelnum)\n",
    "            df_val=pullalldb(dist_df,labelnum)\n",
    "            otherpr=[g for g in groupind if g != k]\n",
    "            for j in otherpr:\n",
    "                bnk=infoarray[j,0]\n",
    "                did=infoarray[j,1]\n",
    "                bnkinds=bf.find(df_val[:,0],bnk)\n",
    "                idinds=bf.find(df_val[:,1],did)\n",
    "                rm = list(set(bnkinds) & set(idinds))\n",
    "                # if rank is 0, it means distance was zero, proper entry is rank 1\n",
    "                if rm == [0]: rm = [1]\n",
    "                rank_match.extend(rm)\n",
    "            all_match.append(rank_match)\n",
    "            match_dist.extend(list(df_val[rank_match[:],2]))\n",
    "            match_dist_form.append(list(df_val[rank_match[:],2]))\n",
    "            nonpair_dist.extend([df_val[i,2] for i in range(len(set(dist_df))) if i not in rank_match])\n",
    "            labelnum=labelnum+1\n",
    "    \n",
    "    # find % of time that known match is closest\n",
    "    total_cnt=[]\n",
    "    closest_match=[]\n",
    "    for rm in all_match:\n",
    "        total_cnt.append(len(rm))\n",
    "        if len(rm)==2:\n",
    "            if rm[0] == 1 and rm[1] == 2:\n",
    "                closest_match.append(2)\n",
    "            elif rm[0]==1 and rm[1]>2:\n",
    "                closest_match.append(1)\n",
    "            else:\n",
    "                closest_match.append(0)\n",
    "        elif len(rm)==1:\n",
    "            if 1 in rm:\n",
    "                closest_match.append(1)\n",
    "            else:\n",
    "                closest_match.append(0)            \n",
    "\n",
    "    five_match=[]\n",
    "    for rm in all_match:\n",
    "        rms = list(np.sort(rm))\n",
    "        if len(rms)==2:\n",
    "            if rm[0]<6 and rm[1]<6:\n",
    "                five_match.append(2)\n",
    "            elif rm[0]<6:\n",
    "                five_match.append(1)\n",
    "            else:\n",
    "                five_match.append(0)\n",
    "        #elif len(rm)==2:\n",
    "        if len(rms)==1:\n",
    "            if rm[0]<6:\n",
    "                five_match.append(1)\n",
    "            else:\n",
    "                five_match.append(0)    \n",
    "                \n",
    "    # count any match top ranked for each donor listing, groups of 2 do not need 2nd to be top ranked also\n",
    "    # balances average over listings\n",
    "    temp=[min(a) for a in all_match]\n",
    "    sing_closest_perc=temp.count(1)/len(temp)\n",
    "    print('a matching donor is top ranked ' + str(100*sing_closest_perc) + '% of the time, by individual donor')\n",
    "    \n",
    "    # count percent top ranked, requiring 1/2 rank for groups of 3\n",
    "    closest_perc = np.sum(closest_match)/np.sum(total_cnt)\n",
    "    print('a matching donor is top ranked ' + str(100*closest_perc) + '% of the time, counting all in groups of 3')\n",
    "    \n",
    "    # find % of time that a known match is in the top 5\n",
    "    topfive_perc = np.sum(five_match)/np.sum(total_cnt)\n",
    "    print('a matching donor is ranked in the top 5 ' + str(100*topfive_perc) + '% of the time, counting all in groups of 3')\n",
    "    \n",
    "    print('mean pair distance is ' + str(np.mean(match_dist)))\n",
    "    print('mean nonpair distance is ' + str(np.mean(nonpair_dist)))\n",
    "    print('distance between pair and nonpair mean is ' + str(np.mean(nonpair_dist)-np.mean(match_dist)))\n",
    "    \n",
    "    return(all_match, match_dist_form, match_dist, closest_perc, topfive_perc, np.mean(match_dist), np.mean(nonpair_dist), np.mean(nonpair_dist)-np.mean(match_dist), sing_closest_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a matching donor is top ranked 55.172413793103445% of the time, by individual donor\n",
      "a matching donor is top ranked 44.2857142857% of the time, counting all in groups of 3\n",
      "a matching donor is ranked in the top 5 51.4285714286% of the time, counting all in groups of 3\n",
      "mean pair distance is 2.5617590039\n",
      "mean nonpair distance is 3.05038175284\n",
      "distance between pair and nonpair mean is 0.488622748937\n"
     ]
    }
   ],
   "source": [
    "# This counts top ranked for 1 & 2 in groups of three\n",
    "CV = eval_model_fulldb(dist_df_db_cv, prs_x, prs_dinfo, prs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make database of distances for full model trained on full training set\n",
    "fullmod_all_dict={}\n",
    "dinfo_dict={}\n",
    "infoarray=np.array(prs_adinfo)\n",
    "dinfo_dict['bankid']=infoarray[:,0]\n",
    "dinfo_dict['donorid']=infoarray[:,1]\n",
    "fullmod_all_df=pd.DataFrame.from_dict(dinfo_dict)\n",
    "for i in range(len(prs_y)):\n",
    "    label='dist'+str(i)\n",
    "    fullmod_all_dict[label]=dist_all[i]\n",
    "temp_df=pd.DataFrame.from_dict(fullmod_all_dict)\n",
    "fullmod_all_df=pd.concat([fullmod_all_df, temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a matching donor is top ranked 94.82758620689656% of the time, by individual donor\n",
      "a matching donor is top ranked 74.2857142857% of the time, counting all in groups of 3\n",
      "a matching donor is ranked in the top 5 100.0% of the time, counting all in groups of 3\n",
      "mean pair distance is 1.17763450522\n",
      "mean nonpair distance is 2.77388302672\n",
      "distance between pair and nonpair mean is 1.5962485215\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of fully trained model in the complete database\n",
    "FM = eval_model_fulldb(fullmod_all_df, prs_x, prs_dinfo, prs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make database of distances for euclidean distance\n",
    "eucl_all_dict={}\n",
    "dinfo_dict={}\n",
    "infoarray=np.array(prs_adinfo)\n",
    "dinfo_dict['bankid']=infoarray[:,0]\n",
    "dinfo_dict['donorid']=infoarray[:,1]\n",
    "#del temp_df\n",
    "eucl_all_df=pd.DataFrame.from_dict(dinfo_dict)\n",
    "for i in range(len(prs_y)):\n",
    "    label='dist'+str(i)\n",
    "    eucl_all_dict[label]=eucl_all[i]\n",
    "temp_df=pd.DataFrame.from_dict(eucl_all_dict)\n",
    "eucl_all_df=pd.concat([eucl_all_df,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a matching donor is top ranked 31.03448275862069% of the time, by individual donor\n",
      "a matching donor is top ranked 24.2857142857% of the time, counting all in groups of 3\n",
      "a matching donor is ranked in the top 5 27.1428571429% of the time, counting all in groups of 3\n",
      "mean pair distance is 9.3745617691\n",
      "mean nonpair distance is 5.31466876414\n",
      "distance between pair and nonpair mean is -4.05989300496\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance of euclidean distance on the full database\n",
    "EU = eval_model_fulldb(eucl_all_df, prs_x, prs_dinfo, prs_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine model performance on test data (shared samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Test data  from banks known to share samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sperm banks have shared samples. Using this information, we can generate a test data set by identifying intersecting IDs, and then discarding pairs with duplicate descriptions. Banks known to share samples are CryosNY and Cryos International, TSBC and NoCASB (name change), Heredity Choice and Repository for Germinal Choice,  European Sperm Bank USA and CCB, Can-Am Cryo and Fairfax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cryos_dup = set.intersection(set(Counts['CryosNY']['Unq_Donors']), set(Counts['Cryos']['Unq_Donors']))\n",
    "# Remove donors who don't have description at one or the other bank\n",
    "removeCryos=['2771FINN', '5524', '1703', 'UGGER', 'GREY']\n",
    "for k in removeCryos:\n",
    "    Cryos_dup.remove(k) \n",
    "# Remove donors who have completely duplicate descriptions\n",
    "ident_Cryos=['2552NILS', '6205DREW','DARIN','5586', '3268UFFE', '137']\n",
    "for k in ident_Cryos:\n",
    "    Cryos_dup.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TSBC_dup = set.intersection(set(Counts['TSBC']['Unq_Donors']), set(Counts['NoCASB']['Unq_Donors']))\n",
    "# Remove donors who don't have description at one or the other bank\n",
    "removeTSBC=['989', '1200']\n",
    "for k in removeTSBC:\n",
    "    TSBC_dup.remove(k)\n",
    "# remove those with totally identical listings\n",
    "ident_TSBC=['741']\n",
    "for k in ident_TSBC:\n",
    "    TSBC_dup.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ESB_dup = set.intersection(set(Counts['ESB']['Unq_Donors']), set(Counts['CCB']['Unq_Donors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Canam_dup = set.intersection(set(Counts['Canam']['Unq_Donors']), set(Counts['Fairfax']['Unq_Donors']))\n",
    "# Remove donors who don't have description at one or the other bank\n",
    "removeCanam=['1887', '2729', '2192', '2175', '2046', '1915', '1911']\n",
    "for k in removeCanam:\n",
    "    Canam_dup.remove(k)\n",
    "# remove totally identical listings\n",
    "ident_Canam=['2398']\n",
    "for k in ident_Canam:\n",
    "    Canam_dup.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HC_dup = set.intersection(set(Counts['HC']['Unq_Donors']), set(Counts['RepGerm']['Unq_Donors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of duplicate IDs with bank info\n",
    "dup_input=[]\n",
    "dup_input.append([Cryos_dup, 'CryosNY', 'Cryos'])\n",
    "dup_input.append([TSBC_dup, 'TSBC', 'NoCASB'])\n",
    "dup_input.append([ESB_dup, 'ESB', 'CCB'])\n",
    "dup_input.append([Canam_dup, 'Canam', 'Fairfax'])\n",
    "dup_input.append([HC_dup, 'HC', 'RepGerm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to output pair listings for each entry in dup_input\n",
    "def makeduppairs(duplist, bank1, bank2):\n",
    "    pair_out=[]\n",
    "    for i in duplist:\n",
    "        p1 = (bank1, i)\n",
    "        p2 = (bank2, i)\n",
    "        ind_pr=[p1, p2]\n",
    "        pair_out.append(ind_pr)\n",
    "    return pair_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make listing of test-data pairs from shared samples, in same format as \"groups\" above\n",
    "test_pairs=[]\n",
    "for i in range(len(dup_input)):\n",
    "    test_pairs.extend(makeduppairs(dup_input[i][0], dup_input[i][1], dup_input[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "CryosNY 1421DANE\n",
      "['Stopped donating 2007', 'Blonde hair', 'Height: 180', 'Started donating Aug 2007', 'Caucassian', 'Danish/Scandinavian', 'Weight: 180 lbs', 'Blood type A+', 'Danish', 'Engineer.', 'Stopped donating Aug 2007', 'BS, Engineering.', 'Weight: 180', \"Height: 6'\", 'Fair complexion', 'Blue eyes', 'Fair complexion.', 'Weight: 6', 'Blonde Wavy hair', \"Height: 6 '0\", 'Born 1966']\n",
      "Cryos 1421DANE\n",
      "['Height: 6\\'0\"', 'Would love to hear from other siblings/parents and of course one day from the donor who gave the biggest gift ever, the gift of life', 'DANISH/SCANDANAVIAN', 'Weight: 180 lbs', 'Engineer.', 'Engineer', 'I have limited information on the donor as it was pre law change and UK do not supply as much information as other countries allow', 'Weight: 180 pounds', 'Please get in touch so I can share info to ensure a match X', 'Stopped donating 2007', \"Height: 6'0\", 'Donor is married with 2 sons.', 'Started donating Aug 2007', 'Stopped donating 2006', 'I have a wonderful 7 year old son from this donor', 'Blood type A+', 'BS, Engineering.', \"Height: 6'\", 'Blue eyes', 'Fair complexion.', 'Weight: 180', 'Building materials', 'Weight: 180lbs', 'Interests: Sports, music', 'Blonde Wavy hair', 'Born 1966', 'Height: 6', 'Blonde Straight hair', 'Danish/Scandinavian', 'Blue eyes.', 'Fair complexion', '(PLEASED TO UPDATE THAT I CAN CONFIRM THAT MY DONOR HAS BEEN PROVEN BY DNA TO BE DANE 1421).', 'The donor has children of his own and donated to Cyros in Denmark', 'Blonde hair', 'I imported to The Midland Fertility clinic in UK', 'Caucassian', 'Danish', 'Stopped donating Aug 2007', 'University', 'Religion:Other', 'Scandinavian', 'ENGINEER', \"Height: 6 '0\", 'I have a sure way of identification as the donor wrote a \"saying\" in with his profile']\n",
      "-----\n",
      "CryosNY 27ERIK\n",
      "['Graduated in 2002 with a Bachelor degree in electrical engineering.', 'Born 1979', 'Weight: 76 kg', 'BS Student (Engineering).', 'Light Brown hair', 'Dark Brown Straight hair', 'Caucasian-Scandinavian', 'Graduated in 2002 - bachelor in electrical engineering.', 'Brown eyes', 'Blood type A-', 'Fair complexion', 'Scandinavian', \"Height: 5'11''\", 'Caucasian', 'Caucasian/Scandinavian', 'Height: 182 cm', 'Weight: 167', 'Engineering.', 'Weight: 167 lbs']\n",
      "Cryos 27ERIK\n",
      "['Graduated in 2002 with a Bachelor degree in electrical engineering.', 'Started donating 1999', 'Born 1979', 'Weight: 76 kg', 'BS Student (Engineering).', 'Light Brown hair', \"Height: 5'11''\", 'Dark Brown Straight hair', 'Stopped donating 2003', 'Graduated in 2002 - bachelor in electrical engineering.', 'Brown eyes', 'Blood type A-', 'Fair complexion', 'Scandinavian', 'Caucasian/Scandinavian', 'Height: 182 cm', 'Weight: 167', 'Engineering.', 'Weight: 167 lbs']\n",
      "-----\n",
      "CryosNY 2508ELIF\n",
      "['Student (natural science).', 'Light Brown Straight hair', 'M', 'S', 'natuaral sciences', 'Has been in burkina faso in his university time and wanted to help suffering people in the third world.', 'Blood type A+', 'Danish', 'Height: 194 cm', 'Academic', 'Born 1974', 'Brown Straight hair', 'Brown eyes', 'Scandinavian', 'M.S', 'Weight: 77 kg']\n",
      "Cryos 2508ELIF\n",
      "['Student (natural science).', 'M', 'M.s.natural science', 'Weight: 77 kg/169 lbs', 'S', 'Dark Brown Straight hair', 'Blood type A+', 'Danish', 'Height: 194 cm', 'Academic', 'Born 1974', 'Brown Straight hair', 'Brown eyes', 'Worked in burkina faso for a while.', 'Fair complexion', 'Scandinavian', 'Weight: 77 kg']\n",
      "-----\n",
      "CryosNY 3836\n",
      "['BS Student Marine Engineer.', 'Brown hair', 'Blood type A+', 'Started donating Nov 1, 2004', 'Blue eyes']\n",
      "Cryos 3836\n",
      "['Brown hair', 'Weight: 87', 'Height: 183', 'Blood type A+', 'BS Student Marine Engineering.', 'Blue eyes', 'Caucasian', 'Started donating Nov 1, 2004']\n",
      "-----\n",
      "CryosNY 927STIG\n",
      "['Psychology.', 'Weight: 192 lb', 'Brown Straight hair', 'Danish', 'Fair complexion', 'Grey eyes.', 'Blue eyes', 'Born 1969', 'Height: 190', 'Blood type O+']\n",
      "Cryos 927STIG\n",
      "['Brown hair', 'Psychology.', 'Weight: 192 lb', 'Brown Straight hair', 'Danish', 'Blue eyes.', 'Fair complexion', 'Blue eyes', 'Born 1969', 'Height: 190', 'Blood type O+']\n",
      "-----\n",
      "CryosNY 786\n",
      "['Weight: 172lbs', 'Blonde Straight hair', 'Height: 6,2', 'Danish', 'Born 1975', 'Blue eyes', 'Fair complexion.', 'Blood type O+']\n",
      "Cryos 786\n",
      "['Blonde hair', 'Started donating 1999', 'Blonde Straight hair', 'Weight: 78 kg', 'Height: 190 cm', 'Height: 190 cm/ 6,23 ft', 'Blue eyes.', 'Born 1975', 'Fair complexion', 'Scandinavian', 'Blue eyes', 'Weight: 78 kg/ 172 lbs', 'Medical School.', 'Blood type O+']\n",
      "-----\n",
      "CryosNY 869TORE\n",
      "['Blonde hair', 'Extended profile.', 'Blood type A+', 'Born 1975', 'Scandinavian']\n",
      "Cryos 869TORE\n",
      "['Blonde hair', 'Danish', 'Blood type A+', 'Height: 175', 'Born 1975', 'Weight: 76', 'Extended profile.']\n",
      "-----\n",
      "CryosNY 3291ANTE\n",
      "['Blonde Straight hair', 'Born 1978', 'Weight: 183', 'Also known as 3291.', 'Blood type A+', 'Height: 6\\' 4\"', 'Fair complexion', 'Blue eyes', 'Caucasian/Scandinavian']\n",
      "Cryos 3291ANTE\n",
      "['Height: 6\\'4\"', 'Caucasian/Scandinavian', 'Height: 6-4', 'Weight: 183', 'Stopped donating 2006', 'Brown Straight hair', 'Blood type A+', 'Height: 6\\' 4\"', 'Blue eyes', 'Fair complexion.', 'No Religion (Atheist).', 'Blonde Straight hair', 'Light Brown Straight hair', 'Height: 1,93', 'Also listed with donor number 3951.', 'Fair complexion', 'Caucasian', 'Pre Med.', 'Student', 'Born 1978', 'Danish', 'Started donating 2001', 'Attending Medical School']\n",
      "-----\n",
      "CryosNY 4688\n",
      "['Weight: 154', '( From Cryos International, Aarhus, Danmark ).', 'Blood type A+', 'Skandinavian', \"Height: 6'2\", 'Anonymous and caucasian', 'Key account manager', 'Brown hair']\n",
      "Cryos 4688\n",
      "['Weight: 154', 'Anonymous and caucasian.', 'Height: 6`2', 'Blood type A+', 'Scandinavian', 'Key account manager', 'Brown hair']\n",
      "-----\n",
      "CryosNY GEIR\n",
      "['Scandinavian', 'Born 1976', 'Medical school', 'Height: 6\\'3\"', 'Blood type A+', 'Brown Straight hair', 'Medical student', 'Weight: 180', 'Fair complexion', 'Height: 6 \\'3\"', 'Blue eyes', 'Caucasian/Scandinavian', 'Elite athlete in track and field.', 'Training in medicine when donating.']\n",
      "Cryos GEIR\n",
      "['Height: 6\\' 3\"', 'Scandinavian', 'Training in medicine.', 'Born 1976', 'Medical school', 'Height: 6\\'3\"', 'Blood type A+', 'Brown Straight hair', 'Weight: 180', 'Fair complexion', 'Blue eyes', 'Caucasian/Scandinavian', 'Elite athlete in track and field.', 'Doctor']\n",
      "-----\n",
      "CryosNY 3949\n",
      "['Blonde hair', 'Blood type O-', 'Weight: 70 kg', 'Started donating Mar 21, 2005', 'Music teacher.', 'Height: 180 cm', 'Blue eyes', 'Born 1970']\n",
      "Cryos 3949\n",
      "['Blonde hair', 'Blood type O-', 'Weight: 70 kg', 'Started donating Mar 21, 2005', 'Music teacher.', 'Blue eyes', 'Born 1970', 'Height: 184 cm']\n",
      "-----\n",
      "CryosNY 821OLAF\n",
      "['Blonde hair', 'Student of Psychology.', 'Started donating Jun 25, 1999', 'Caucasian / scandinavian', 'Blood type A+', 'Stopped donating Aug 11, 2000', 'Weight: 65 kg', 'Born 1975', 'Height: 183 cm', 'Blue eyes']\n",
      "Cryos 821OLAF\n",
      "['Height: 6\\'0\"', 'Student of Psychology.', 'Protestant.', 'Business', 'Donors alias: Olaf.', 'Height:', 'PSD Psychology?', 'Brown Straight hair', 'Blood type A+', 'Caucasian / scandinavian', 'Stopped donating Aug 11, 2000', 'Weight: 143 lbs', 'Blue eyes', 'Blonde Straight hair', 'Masters', 'Christian', 'Blue eyes.', 'Height: 183 cm', 'Caucasian', 'Weight: 143', '/ 143 ibs', 'Blonde hair', 'Danish', 'Started donating Jun 25, 1999', 'Weight: 65 kg', 'Born 1975']\n",
      "-----\n",
      "CryosNY 782TIMM\n",
      "['Height: 178 (5\\'10\")', 'Studying Medicine', 'Started donating Jan 2004', 'Born 1976', 'Looking for ties with this donor, unique in that 782 \"TIMM\" is Vietnamese and lives/lived in Denmark-he has traveled to Saigon to meet up with past family there and dreams of giving something back to his birth country Vietnam.He was a medical student.', 'Black Straight hair', 'Medium complexion', 'Brown eyes', 'Vietnamese', 'Graduated high school 1994 US, Danish high school exam in 1996, Graduate university 2005 with PhD', 'Blood type B+', 'Weight: 76 (168 lbs)', 'Medical Student/Doctor']\n",
      "Cryos 782TIMM\n",
      "['Height: 178 (5\\'10\")', 'Studying Medicine', 'Started donating Jan 2004', 'Born 1976', 'Looking for ties with this donor, unique in that 782 \"TIMM\" is Vietnamese and lives/lived in Denmark-he has traveled to Saigon to meet up with past family there and dreams of giving something back to his birth country Vietnam.He was a medical student.', 'Black Straight hair', 'Medium complexion', 'Brown eyes', 'Vietnamese', 'Fair complexion', 'Graduated high school 1994 US, Danish high school exam in 1996, Graduate university 2005 with PhD', 'Blood type B+', 'Agnostic', 'Weight: 76 (168 lbs)', 'Medical Student/Doctor']\n",
      "-----\n",
      "CryosNY 997SVEN\n",
      "['Born 1960', 'Just email the donor sibling registry.', 'Blonde Straight hair', 'Weight: 141 lbs', 'Christian', 'If donor wishes to join, we are happy to pay the fee', 'Studying photography/Organ player', 'Blood type A-', 'Height:', 'Blue eyes', 'Fair complexion', 'Caucasian']\n",
      "Cryos 997SVEN\n",
      "['Born 1960', 'studying photography', 'Just email the donor sibling registry.', 'Blonde Straight hair', 'Weight: 141 lbs', 'He has two children of his own', 'Christian', 'If donor wishes to join, we are happy to pay the fee', \"Height: 5'6\", 'Allergic to grass-pollen', 'Blood type A-', 'Fair complexion', 'Blue eyes', 'Plays many instruments', 'Caucasian', 'He was in the airforce for 9 months', 'Organ player']\n",
      "-----\n",
      "CryosNY 1146\n",
      "['Scandinavian', 'Weight: 69kg', 'Blood type A+', 'Height: 178cm', 'Blue eyes.', 'Weight: 69 kg', 'Height: 178', 'Dark Brown hair', 'Weight: 69', 'Height: 178 cm', 'Blue eyes', 'Born 1967', 'Nordic', 'Brown hair']\n",
      "Cryos 1146\n",
      "['Scandinavian', 'Blood type A+', 'Weight: 69 kg', 'Dark Brown hair', 'Blue eyes.', 'Height: 178', 'Weight: 69', 'Height: 178 cm', 'Blue eyes', 'Born 1967', 'Brown hair']\n",
      "-----\n",
      "CryosNY 3088KNUT\n",
      "['Blonde Straight hair', 'Danish', 'Archiology.', 'Fair complexion', 'Blue eyes', 'Msc']\n",
      "Cryos 3088KNUT\n",
      "['Blonde hair', 'Danish', 'Blue eyes.']\n",
      "-----\n",
      "CryosNY 006ARNE\n",
      "['Scandinavian.']\n",
      "Cryos 006ARNE\n",
      "['Scandinavian.', 'Born 1977']\n",
      "-----\n",
      "CryosNY 672DANN\n",
      "['Weight: 170', 'Masters Degree', 'Blood type O-', 'Media', 'Brown Straight hair', 'Medium complexion', 'Caucasian, Danish', 'Brown eyes', 'Height: 6-1', 'Media Science.', 'Scandinavian', 'Height: 6\\' 1\"', 'Brown eyes.', 'Christian.', 'Master Degree', 'Born 1972', 'Brown hair']\n",
      "Cryos 672DANN\n",
      "['Journalist', \"Height: 6'1\", 'Weight: 170', 'Height: 6\\'1\"', 'Blood type O-', 'Speaks several languages, has one sister, both parents were teachers.', 'Dark Brown Straight hair', 'Danish', 'Brown Straight hair', 'Masters Degree Media Science', 'Medium complexion', 'Christian', 'Brown eyes', 'Caucasian', 'Christian.', 'Born 1972']\n",
      "-----\n",
      "CryosNY 3778\n",
      "['Blue eyes', 'Computer engineering.', 'Brown hair', 'Ms', 'Scandinavian']\n",
      "Cryos 3778\n",
      "['Brown hair', 'Blue eyes', 'Height: 192 cm', 'MSc Computer Engineering.', 'Scandinavian']\n",
      "-----\n",
      "CryosNY MATT\n",
      "['Brown Straight hair', 'Blood type A+', 'Oral Surgeon', 'Doctorate in Dental Surgery', 'Height: 6 feet', 'Fair complexion', 'LOOKING FOR VIALS!.', 'Blue eyes', 'Born 1982', 'Weight: 165 pounds']\n",
      "Cryos MATT\n",
      "['Brown Straight hair', 'Blood type A+', 'Oral Surgeon', 'LOOKING FOR VIALS.', 'Doctorate in Dental Surgery', 'Height: 6 feet', 'Fair complexion', 'Blue eyes', 'Born 1982', 'Weight: 165 pounds']\n",
      "-----\n",
      "CryosNY 3001JENS\n",
      "['Brown hair', 'Height: 6\\'1\" (185cm)', 'Student.', 'Born 1979', 'Blood type A+', 'Danish', 'Studied Masters Physical Chemistry at university', 'Graduated from high school in 1999', 'Has 1 younger brother and his mother is English.', 'Fair complexion', 'Blue eyes', 'Weight: 73 kg', 'Also known as \"JENS\"', 'Height: 187 cm', 'Blonde Curly hair', 'Weight: 175lbs (80kg)']\n",
      "Cryos 3001JENS\n",
      "['Brown hair', 'Height: 6\\'1\" (185cm)', 'Student.', 'Born 1979', 'Studied for an MSc in Physical Chemistry at university', 'Donor also known as \"Jens\"', 'Has one younger brother', 'Mother was English.', 'Blood type A+', 'Danish', 'Graduated from high school in 1999', 'Fair complexion', 'Blue eyes', 'Weight: 73 kg', 'Blonde Wavy hair', 'Height: 187 cm', 'Weight: 175lbs (80kg)']\n",
      "-----\n",
      "CryosNY 704TYGE\n",
      "['Blood type O-', \"Have recently found an extended profile on this donor that I'm happy to share.\", 'Brown Straight hair', 'Started donating Mar 11, 1997', 'Weight: 75 kg', 'Height: 180 cm', 'Fair complexion', 'Scandinavian']\n",
      "Cryos 704TYGE\n",
      "['I am happy to share this.', 'Blood type O-', 'Brown Straight hair', 'Weight: 75 kg', 'Started donating Mar 11, 1999', 'Height: 180 cm', \"I have recently found an extended profile on the donor from January 2004 which we didn't have at the time of conception\", 'Fair complexion', 'Scandinavian']\n",
      "-----\n",
      "TSBC 16\n",
      "['Degree', 'Blood type A+', 'English/Irish/German', 'Database analyst.', 'Height: 5\\'11\"', 'Fair complexion', 'Blue eyes', 'Weight: 180', 'Blonde Wavy hair', 'Born Jan 1953']\n",
      "NoCASB 16\n",
      "['Degree', 'Blood type A+', 'English/Irish/German', 'Database analyst.', 'Fair complexion', 'Blue eyes', 'Weight: 180', 'Height: 5\\' 11\"', 'Born Jan 1953', 'Blonde Wavy hair']\n",
      "-----\n",
      "TSBC 2001\n",
      "['Caucasian', 'Born 1951', 'Weight: 159.', 'Height: 5\\'8\"']\n",
      "NoCASB 2001\n",
      "['Born Jun 1951.']\n",
      "-----\n",
      "TSBC 708\n",
      "['Born May 1967', 'Started donating 1990', 'Born May 23', 'Economics', 'Clinic said he was very funny and smart.', 'Music.', 'Blood type A+', 'B.A', 'B.S', \"Height: 5'11\", 'Blue eyes', 'Hazel eyes', 'Blonde Straight hair', 'Musician', 'Methodist.', 'Fair complexion', 'Height: 5\\'10\"', 'Blonde hair', 'Weight: 145', 'Weight: 128', 'Dutch, English', 'BA Economics, Graduate studies English']\n",
      "NoCASB 708\n",
      "['Blonde hair', 'Musician', 'Blood type A+', 'Weight: 128', 'Dutch, English', 'Born May 1967', 'Fair complexion', 'BA Economics, Graduate studies English', 'Methodist', '', 'Hazel eyes', 'Started donating 1990', 'Height: 5\\'10\"']\n",
      "-----\n",
      "TSBC 8\n",
      "['English/Welsh/Noregian', 'Blonde hair', 'Height: 6\\'1\"', 'Light Brown Straight hair', 'Blood type O-', 'German.', 'Started donating 1982', 'Born 1959', 'Weight: 200#', 'College-Economics', 'Blue eyes.', 'Fair complexion', 'Baptist.', 'Born Nov 11, 1959', 'Stopped donating 1985', 'Blue eyes', 'Caucasian']\n",
      "NoCASB 8\n",
      "['Height: 6\\'1\"', 'Light Brown Wavy hair', 'Protestant.', 'Started donating 1982', 'Born Dec 1959', 'English/German', 'Weight: 200#', 'College-Economics', 'Fair complexion', 'Blue eyes']\n",
      "-----\n",
      "TSBC 355\n",
      "['Had never been a sperm donor before', 'Had never served in the military', 'Brown Wavy hair', 'He was the oldest of four children, with one brother and two sisters.', 'Born Oct 1953', 'Hazel eyes', 'Roman Catholic', 'Employed in house construction in 1987', 'Additional information from long chart provided on donor: he was single in 1987, age 33 at the time he completed the interview', 'Medium body type', 'Exercised regularly with non-contact sports; hang gliding, frisbee, volleyball, swimming', 'Had no children at time, but had been responsible for a pregnancy in 1971 (chart provided no further information on this pregnancy)', 'Completed college, degree in poli sci in 1976', 'Fair complexion', 'Poor vision; he was nearsighted and wore glasses', 'Weight: 150 #', 'English/Irish', 'Had a routine appendectomy in 1970', 'Height: 5\\'10\"', 'Blood type O+']\n",
      "NoCASB 355\n",
      "['Brown Wavy hair', 'Born Oct 1953', 'Hazel eyes', 'Employed in house construction in 1987', 'Height: 5\\' 10\"', 'Completed college, degree in poli sci in 1976', 'Fair complexion', 'Roman Catholic.', 'Weight: 150 #', 'English/Irish', 'Blood type O+']\n",
      "-----\n",
      "TSBC 1258\n",
      "['Light Brown Straight hair', 'Blood type O-', 'Weight: 168', 'Born Apr 1971', 'Medium complexion', \"Height: 6'\", 'Hazel eyes', 'Christian.']\n",
      "NoCASB 1258\n",
      "['Light Brown Straight hair', 'Blood type O-', 'Weight: 168', 'Born Apr 1971', 'Height: 6\"', 'Medium complexion.', 'Hazel eyes']\n",
      "-----\n",
      "ESB 7093\n",
      "['Blonde Straight hair', 'Blood type O-', 'Height: 6`5`', 'Philosophy Student', 'Fair complexion', 'Caucasian', 'Blue eyes', 'AKA \"Georg\".', 'Born 1982', 'Weight: 177 lbs']\n",
      "CCB 7093\n",
      "['Student', 'Blonde Straight hair', 'Blood type O-', 'Height: 6`5`', 'Aka \"Georg\".', 'Philosophy Student', 'Fair complexion', 'Caucasian', 'Blue eyes', 'Born 1982', 'Weight: 177 lbs']\n",
      "-----\n",
      "Canam 2181\n",
      "['See Fairfax #2181.', 'Blood type A+', 'Height: 5\\'11\"', 'Weight: 185', 'Fair complexion', 'Blue eyes', 'German/Finnish', 'Blonde Curly hair']\n",
      "Fairfax 2181\n",
      "['Blonde hair', 'Other pregnancies reported', 'Student in Anthrolopology', 'Well travelled and well read.', 'Mentions James Joyce and Ulysses in his audio', 'Blood type A+', 'German', 'German/Finnish', 'Medium complexion', 'BA ENGLISH', 'Height: 5\\'11\"', 'TEACHER', 'Caucasian', 'Blue eyes', 'Fair complexion.', 'Blonde Curly hair', 'Christian.', 'BA In English and French', 'Weight: 185', 'Blonde Wavy hair']\n",
      "-----\n",
      "Canam 1084\n",
      "['Green eyes', 'Blonde Straight hair', \"Height: 5'10''\", 'Dutch/english', 'Fair complexion.', 'Weight: 180', 'Blood type AB+']\n",
      "Fairfax 1084\n",
      "['English/Dutch', 'Green eyes', 'Blonde Straight hair', 'Dutch/English', 'Weight: 145 lbs', 'Several half siblings reported.', 'Weight: 145', 'Height: 5\\' 10\"', 'Fair complexion', 'Fair complexion.', 'Donor had a child (girl, age 7 at time of donation).', 'Height: 5\\'10\"', 'Blood type AB+']\n",
      "-----\n",
      "Canam 1763\n",
      "['Brown Wavy hair', 'Light Brown Wavy hair', 'Blood type O-', 'Protestant.', 'German', 'Fair complexion', 'Blue eyes', 'Christian.']\n",
      "Fairfax 1763\n",
      "['German', 'Protestant.', 'Fair complexion', 'Brown Wavy hair', 'Blue eyes']\n",
      "-----\n",
      "Canam 2918\n",
      "[\"Height: 6'0\", 'Green eyes', 'Blonde Straight hair', 'Norwegion', 'Weight: 184', 'Height: 6.0', 'Medium complexion', 'Finnish/norwegian', 'University-Bachelor of Engineering.', 'Masters Engineering.', 'Blood type O+']\n",
      "Fairfax 2918\n",
      "['Height: 6\\'0\"', 'Medium complexion', 'Engineer', 'Blood type O+', 'Finnish/Norwegian', \"Height: 6' 0\", \"Height: 6'0\", 'BA Engineering', 'Finnish', 'In the USMC, Favorite song is Beast of Burden.', 'Christian.', 'MA/Engineering', 'BA/Engineering', 'Masters Engineering.', 'Caucasion', 'Height: 6\\' 0\"', 'Height: 6', 'Green eyes', 'Blonde Straight hair', 'Student.', 'Donor 2918 is attractive with Nordic features: blond hair and gorgeous pale green eyes.', 'Height: 6.0', 'Christian', 'Seeking BA/ Engineering', 'Masters in engineering', 'Fair complexion', 'Caucasian', 'Masters in Engineering', 'MA/engineering', 'Green eyes.', 'Weight: 184lbs', 'Height: 6.0\"', 'Blonde hair', 'Student', 'Musical, Team Sports, Individual Sports, Craftsman, Creative/Artistic.', 'Finnish/Norwegian with sizzeled nordic features.', 'MA/ Engineering', 'Weight: 184', 'Finnish/Norwegian-Finnish']\n",
      "-----\n",
      "Canam 523\n",
      "['Blue eyes.', 'Black hair', 'Blood type O+']\n",
      "Fairfax 523\n",
      "['Message for the parents of 0523 children.', 'Black hair', 'BS Anthropology.', 'BS/Anthropology.', 'Black Straight hair', 'Blue eyes.', 'Scottish/Irish', 'Fair complexion', 'Blue eyes', 'Weight: 180', 'Height: 5\\'10\"', 'Height: 5-10', 'Blood type O+']\n",
      "-----\n",
      "Canam 78\n",
      "['Brown Wavy hair', 'Video Editor.', 'Welsh/Irish', 'Weight: 165', 'Brown eyes', 'Olive complexion', 'BS Communications', 'Height: 5\\'10\"', 'Blood type O+']\n",
      "Fairfax 78\n",
      "['Recording tech.', 'Height: 5 10', 'BS Communication', 'Welsh/irish', 'BS Communications', 'BS communications', '4 years College', 'Blood type O+', 'Video Editor', 'Brown Wavy hair', 'Video Editor.', 'Welsh / Irish', 'Started donating 1987', 'Olive complexion', 'Irish/Welsh', 'Stopped donating 1993', 'Allergic to large doses of penicillin.', 'Caucasian', 'Stopped donating 1989', 'Height: 5\\'10\"', 'Welsh', 'Has two other brothers', '4 yrs college', 'Sales Rep.', 'Welsh/Irish', 'Weight: 165', 'Height: 5\\' 10\"', 'Brown eyes', 'Brown hair.', 'Had separated shoulder at one time', 'Started donating 1989']\n",
      "-----\n",
      "Canam 2149\n",
      "['Height: 6\\'1\"', 'Brown Wavy hair', 'Lawyer', 'Medium complexion', 'Brown eyes', 'Height: 6.1', 'Weight: 180', 'Jewish.', 'Law', 'Blood type O+']\n",
      "Fairfax 2149\n",
      "['Brown Curly hair', \"Height: 6'1\", 'Height: 6\\'1\"', 'Brown Wavy hair', 'PHD', 'Lawyer', 'German Jewish/Russian Jewish', 'RUSSIOAN jEWISH', 'Medium complexion', 'Attorney.', 'Brown eyes', \"Height: 6'?\", 'Height: 6\\'1\".', 'Jewish', 'Height: 6.1', 'Weight: 180', 'Jewish.', 'Blood type O+']\n",
      "-----\n",
      "Canam 2505\n",
      "['Weight: 210', 'Blood type A+', 'Irish/German', 'Attorney.', 'Juris Doctorate', 'Fair complexion', 'Blue eyes', 'Blonde Wavy hair', \"Height: 6'3\"]\n",
      "Fairfax 2505\n",
      "['Catholic.', 'Weight: 210', 'Lawyer', 'Height: 6\\'3\"', 'Law Degree', 'Height: 6ft 3 inches', 'Irish, German', 'Law degree', 'Attorney', 'PH-D', 'Blood type A+', 'Irish/German', 'Started donating Feb 1, 2006', 'Blue eyes', 'Fair complexion.', 'German/irish', 'Originally art director; law student', 'Juris Doctorate', 'Fair complexion', 'Blonde Curly hair', \"Height: 6'3\", 'College', 'Stopped donating Feb 2, 2006', 'Attorney.', 'German/Irish', 'Blonde Wavy hair']\n",
      "-----\n",
      "Canam 1558\n",
      "[\"Height: 6'1\", 'Student', 'Blonde Straight hair', 'Blood type A+', 'Irish/German', 'Irish German', 'Brown eyes', 'Fair complexion', 'Weight: 190', 'Height: 6.1', 'Fair complexion.', 'Wears glasses, Grandparents were 70,72,75 & 76 years old.', 'Computer Science']\n",
      "Fairfax 1558\n",
      "[\"Height: 6'1\", 'Medium complexion', 'Wears glasses', 'College Student', 'Height: 6\\'1\"', 'Computer science.', 'Blood type A+', 'Irish/German', 'Weight: 190', 'Fair complexion.', 'Caucasion', 'Blonde Straight hair', 'Caucasian Irish/German', 'Student in Computer Science.', 'Fair complexion', 'Grandparents were 70, 72, 75 & 76 years old.', 'Caucasian', 'Freshmen Computer Science.', 'Student', 'College graduate', 'No longer available as of 2006.', 'Irish German', 'Brown eyes', 'Was on the donor list as early as 3/92', 'Computer Science']\n",
      "-----\n",
      "Canam 1848\n",
      "['Height: 5\\'9\"', 'Scottish/German', 'Scientist.', 'Weight: 178', 'PhD Ops Research', 'Blue eyes', 'Blood type B+', 'Red Wavy hair']\n",
      "Fairfax 1848\n",
      "['Unitarian.', 'Weight: 170', 'Other.', 'Height: 5\\'9\"', 'Scottish/German', 'Scientist.', 'Weight: 178', 'Scottish, German', 'Scientist', 'PhD Ops Research', 'Blue eyes.', 'German, Scottish', 'PhD', 'Medium complexion', 'German/Scottish', 'Fair complexion', 'Blue eyes', 'Operations Research, USAF', 'Blood type B+', 'Phd Ops Research', 'Red Wavy hair']\n",
      "-----\n",
      "HC GREEN-RED\n",
      "['Music: evidence of good ability Athletics: in college he was a champion in a demanding sport Offspring: show great promise in music and exceptional physical coordination.', 'Northwest European', 'Blood type A+', 'Weight: 163', 'Fair complexion', 'Blue eyes', 'Professor of mathematics', 'Height: 5\\'10\"', 'Blonde Curly hair']\n",
      "RepGerm GREEN-RED\n",
      "['Northwest European', 'Blood type A+', 'Weight: 163', 'Fair complexion', 'Blue eyes', 'Professor of mathematics', 'Music :evidence of good ability In colleage he was a champion in a demanding sport Offspring show exceptional physical coordination and good in music.', 'Height: 5\\'10\"', 'Blonde Curly hair']\n"
     ]
    }
   ],
   "source": [
    "# Clean these pairs as above for the \"groups\" training set\n",
    "for j,p in enumerate(test_pairs):\n",
    "    p_banks=[]\n",
    "    p_ids=[]\n",
    "    print('-----')\n",
    "    listset=[]\n",
    "    for i in [0,1]:\n",
    "        p_bank = p[i][0].strip()\n",
    "        p_id = str(p[i][1])\n",
    "        print(p_bank, p_id)\n",
    "    \n",
    "        if bf.find(Counts[p_bank]['Unq_Donors'],p_id):\n",
    "                        \n",
    "            for i,a in enumerate(DescList[p_bank][p_id]['AllText']):\n",
    "                for rmv in rmv_library:\n",
    "                    if rmv in a:\n",
    "                        DescList[p_bank][p_id]['AllText'][i]=''\n",
    "                        continue\n",
    "                        \n",
    "            print(DescList[p_bank][p_id]['AllText'])\n",
    "            \n",
    "            listset.append(set(DescList[p_bank][p_id]['AllText']))\n",
    "            \n",
    "        else:\n",
    "            print('*** Not in database ***')\n",
    "            \n",
    "    if len((listset[0]-listset[1]))==len(listset[1]-listset[0])==0:\n",
    "        print(\"*** Descriptions Identical -- cut ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data from database and evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data for test pairs from database\n",
    "for i in range(len(test_pairs)):\n",
    "    \n",
    "    donorid1=str(test_pairs[i][0][1])\n",
    "    donorid2=str(test_pairs[i][1][1])\n",
    "    bankid1=str(test_pairs[i][0][0])\n",
    "    bankid2=str(test_pairs[i][1][0])\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM dsr_db6 \n",
    "    WHERE (bankid = '%s' AND donorid = '%s')\n",
    "    OR (bankid = '%s' AND donorid = '%s')\n",
    "    \"\"\" % (bankid1, donorid1, bankid2, donorid2)\n",
    "        \n",
    "    if i == 0:\n",
    "        prs_tp = pd.read_sql_query(query,con)\n",
    "        prs_tp['pairid']=[0]*len(prs_tp)\n",
    "    else:\n",
    "        pr_indiv = pd.read_sql_query(query, con)\n",
    "        pr_indiv['pairid']=[i]*len(pr_indiv)\n",
    "        prs_tp = pd.concat([prs_tp, pr_indiv], axis=0)\n",
    "        \n",
    "# drop columns we are not including as features\n",
    "# prs_tpx are our features\n",
    "prs_tpx=prs_tp\n",
    "prs_tpx = prs_tpx.drop('index', 1)\n",
    "prs_tpx = prs_tpx.drop('offspcnt', 1)\n",
    "prs_tpx = prs_tpx.drop('super', 1)\n",
    "prs_tpx = prs_tpx.drop('alltext', 1)\n",
    "prs_tpx = prs_tpx.drop('bankid', 1)\n",
    "prs_tpx = prs_tpx.drop('donorid', 1)\n",
    "prs_tpx = prs_tpx.drop('pairid', 1)\n",
    "prs_tpx = prs_tpx.drop('eyeexist', 1)\n",
    "prs_tpx = prs_tpx.drop('wordcount', 1)\n",
    "\n",
    "# pair info that we are training on\n",
    "prs_tpy = prs_tp['pairid']\n",
    "\n",
    "# donor info\n",
    "prs_tp_dinfo = pd.concat([prs_tp['bankid'], prs_tp['donorid']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distances over full data set for test_pairs\n",
    "dist_all_tp=[]\n",
    "prs_tpx_df=prs_tpx\n",
    "prs_tpx=np.array(prs_tpx)\n",
    "for i in range(prs_tpx.shape[0]):\n",
    "    dist_temp=[]\n",
    "    for d in range(prs_a.shape[0]):\n",
    "        dist=prs_tpx[i,:]-prs_a[d,:] \n",
    "        lmnn_dist=np.sqrt(np.dot(dist.T,np.dot(W,dist)))\n",
    "        dist_temp.append(lmnn_dist)\n",
    "    dist_all_tp.append(dist_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make database of distances for test pairs for evaluation\n",
    "tp_all_dict={}\n",
    "dinfo_tp_dict={}\n",
    "infoarray=np.array(prs_adinfo)\n",
    "dinfo_tp_dict['bankid']=infoarray[:,0]\n",
    "dinfo_tp_dict['donorid']=infoarray[:,1]\n",
    "tp_all_df=pd.DataFrame.from_dict(dinfo_tp_dict)\n",
    "for i in range(len(prs_tpy)):\n",
    "    label='dist'+str(i)\n",
    "    tp_all_dict[label]=dist_all_tp[i]\n",
    "temp_df=pd.DataFrame.from_dict(tp_all_dict)\n",
    "tp_all_df=pd.concat([tp_all_df,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a matching donor is top ranked 65.0% of the time, by individual donor\n",
      "a matching donor is top ranked 65.0% of the time, counting all in groups of 3\n",
      "a matching donor is ranked in the top 5 83.75% of the time, counting all in groups of 3\n",
      "mean pair distance is 1.19228285427\n",
      "mean nonpair distance is 2.14391319655\n",
      "distance between pair and nonpair mean is 0.951630342283\n"
     ]
    }
   ],
   "source": [
    "TP = eval_model_fulldb(tp_all_df, prs_tpx, prs_tp_dinfo, prs_tpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection for closest match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the online model uses a hard threshold of 2, and a requirement for a sufficient number of words to be included in the description in order to predict a match. But this is not actually ideal, because the distribution of distances from any particular donor can vary quite a bit, so a match may actually have a distance of more than 2 for a donor who tends to have large distance. This is why I return a full list of rank ordered donors, but I'm working on a better way to predict actual matches. One way is to treat each donor's match detection as anomaly detection -- a donor much closer than the norm for that donor is more likely to be a match. This is not yet implemented in the web site, but this section contains some initial work in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_match(samples, thresh):\n",
    "    rev_thresh=np.mean(samples)-thresh*np.std(samples)\n",
    "    if samples[np.argsort(samples)[1]] < rev_thresh:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_dist(samples, titletag, rm):\n",
    "    ax = plt.figure()\n",
    "    binsfixed=np.linspace(0,6,num=25)\n",
    "    weightsfixed = np.ones_like(samples)/len(samples)\n",
    "    plt.hist(samples, alpha=0.3, color='blue', normed=0, bins=binsfixed, weights=weightsfixed)\n",
    "\n",
    "    #detect whether a match is detected and plot X in corner if yes\n",
    "    if detect_match(samples, 3):\n",
    "        ax.text(0.5, 0.5, 'X', fontsize=30)\n",
    "    \n",
    "    for i in range(0,11):\n",
    "        plt.plot([samples[np.argsort(samples)[i]], samples[np.argsort(samples)[i]]],[0, 0.25],'--')\n",
    "    for r in rm:\n",
    "        plt.plot([samples[np.argsort(samples)[r]], samples[np.argsort(samples)[r]]],[0, 0.25], color='black')\n",
    "    plt.title(titletag)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pred_test(df, rm, thresh, numsamps):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    top_ranked_neg = 0\n",
    "    for i in range(0,numsamps):\n",
    "        label='dist'+str(i)\n",
    "        if detect_match(df[label], thresh):\n",
    "            if 1 in rm[i]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "        else:\n",
    "            false_neg += 1\n",
    "            if 1 in rm[i]:\n",
    "                top_ranked_neg += 1\n",
    "    return(true_pos/numsamps, false_pos/numsamps, false_neg/numsamps, top_ranked_neg/numsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_thresh_curves(df, rm, numsamps):\n",
    "    threshlist=list(np.linspace(1,8,32))\n",
    "    true_pos=[]\n",
    "    false_pos=[]\n",
    "    false_neg=[]\n",
    "    top_ranked_neg=[]\n",
    "    for t in threshlist:\n",
    "        temp=pred_test(df, rm, t, numsamps)\n",
    "        true_pos.append(temp[0])\n",
    "        false_pos.append(temp[1])\n",
    "        false_neg.append(temp[2])\n",
    "        top_ranked_neg.append(temp[3]) \n",
    "    return(threshlist, true_pos, false_pos, false_neg, top_ranked_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def thresh_curve_fig(df, rm, numsamps, titletag):\n",
    "    threshlist, true_pos, false_pos, false_neg, top_ranked_neg = gen_thresh_curves(df, rm, numsamps)\n",
    "    top_ranked=[true_pos[i] + top_ranked_neg[i] for i in range(len(threshlist))]\n",
    "    plt.figure()\n",
    "    plt.plot(threshlist, true_pos, color='black')\n",
    "    plt.plot(threshlist, false_pos, color='red')\n",
    "    plt.plot(threshlist, false_neg, color='blue')\n",
    "    plt.plot(threshlist, top_ranked, color='green')\n",
    "    plt.legend(['true_pos','false_pos','false_neg','top_ranked'])\n",
    "    plt.title(titletag)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh_curve_fig(dist_df_db_cv, A[0], 58, 'Cross_Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresh_curve_fig(eucl_all_df, EU[0], 58, 'Euclidean Dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresh_curve_fig(tp_all_df, TP[0], 58, 'Test Pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.argmin(dist_df_db_cv['dist0'])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are functions I used to do recursive feature elimination to reduce the feature space. There is always a danger of overfitting with these techniques, but most of the words kicked out by this method were ones you'd expect to be meaningless or add noise, and I know that I did not overfit based on performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def elimround(prs_x_df, prs_x, prs_y, prs_dinfo, rnd):\n",
    "\n",
    "    feature_names_temp = list(prs_x_df.columns.values)\n",
    "    print(feature_names_temp)\n",
    "    best_features = keep_features = list(range(prs_x.shape[1]))\n",
    "\n",
    "    scores_all=[]\n",
    "    fivescores_all=[]\n",
    "    del_feature=[]\n",
    "    mdist_all=[]\n",
    "    nmdist_all=[]\n",
    "    index_track=[]\n",
    "    feat_name=[]\n",
    "\n",
    "    for i in [0]:\n",
    "        filename='featureelim_'+ str(rnd)\n",
    "        #for i in range(prs_x.shape[1]):\n",
    "        scores = []\n",
    "        fivescores = []\n",
    "        md_temp = []\n",
    "        nmd_temp = []\n",
    "        index_track = []\n",
    "        distdiff_temp = []\n",
    "        #for j in list(range(len(best_features))):\n",
    "        for j in list(range(17, len(best_features))):\n",
    "            print(i,j)\n",
    "            keep_features = best_features[:] \n",
    "            print('remove feature: ' + feature_names_temp[j])\n",
    "            del keep_features[j] \n",
    "            dist_df, W = run_crossval_LMNN(prs_x[:, keep_features], prs_y, prs_dinfo)\n",
    "            top_temp, five_temp, mdist, nmdist, distdiff = eval_model(dist_df, prs_y) \n",
    "            scores.append(top_temp) \n",
    "            fivescores.append(five_temp)\n",
    "            md_temp.append(mdist)\n",
    "            nmd_temp.append(nmdist)\n",
    "            distdiff_temp.append(distdiff)\n",
    "            index_track.extend([i,j])\n",
    "            feat_name.append(feature_names_temp[j])\n",
    "        \n",
    "            with open('picklefiles/' + filename +'.pickle', 'wb') as handle:\n",
    "                pickle.dump((scores, fivescores, md_temp, nmd_temp, distdiff_temp, index_track, feat_name), handle)  \n",
    "        \n",
    "    return (scores, fivescores, md_temp, nmd_temp, distdiff_temp, index_track, feat_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keeporgo(scores, fivescores, distdiff_temp, feat_names, bestsc, bestf, bestdiff, strict, filename):\n",
    "\n",
    "    score_ch=[round(s-bestsc,2) for s in scores]\n",
    "    five_ch=[round(k-bestf,2) for k in fivescores]\n",
    "    diff_ch=[round(d-bestdiff,4) for d in distdiff_temp]\n",
    "\n",
    "    s_keep=[]\n",
    "    f_keep=[]\n",
    "    mn_keep=[]\n",
    "    f_nm_keep=[]\n",
    "    s_go=[]\n",
    "    f_go=[]\n",
    "    mn_go=[]\n",
    "    f_nm_go=[]\n",
    "    s_mid=[]\n",
    "    f_mid=[]\n",
    "    mn_mid=[]\n",
    "    f_nm_mid=[]\n",
    "    for i,z in enumerate(zip(score_ch, five_ch, diff_ch, feat_names)):\n",
    "        if z[0]<0 or z[1]<0 or z[2]<0:\n",
    "            s_keep.append(z[0])\n",
    "            f_keep.append(z[1])\n",
    "            mn_keep.append(z[2])\n",
    "            f_nm_keep.append(z[3])\n",
    "        elif strict == 1 and z[0]>0 and z[1]>0 and z[2]>=0:\n",
    "            s_go.append(z[0])\n",
    "            f_go.append(z[1])\n",
    "            mn_go.append(z[2])\n",
    "            f_nm_go.append(z[3])\n",
    "        elif strict == 0 and z[0]>=0 and z[1]>0 and z[2]>=0:\n",
    "            s_go.append(z[0])\n",
    "            f_go.append(z[1])\n",
    "            mn_go.append(z[2])\n",
    "            f_nm_go.append(z[3])\n",
    "        elif strict == 0 and z[0]>0 and z[1]>=0 and z[2]>=0:\n",
    "            s_go.append(z[0])\n",
    "            f_go.append(z[1])\n",
    "            mn_go.append(z[2])\n",
    "            f_nm_go.append(z[3])    \n",
    "        else:\n",
    "            s_mid.append(z[0])\n",
    "            f_mid.append(z[1])\n",
    "            mn_mid.append(z[2])\n",
    "            f_nm_mid.append(z[3])\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.plot([bestsc, bestsc], [0.75, 0.85])\n",
    "    plt.plot([0.58, 0.7], [bestf, bestf])\n",
    "    sc_r=[s+0.005*random.random() for s in scores]\n",
    "    fs_r=[f+0.005*random.random() for f in fivescores]\n",
    "    plt.scatter(sc_r, fs_r)\n",
    "    plt.xlabel('% top rank', fontsize=18)\n",
    "    plt.ylabel('% top five', fontsize=18)\n",
    "    plt.show()\n",
    "    plt.savefig('figures/Elimination_2_score.pdf')\n",
    "    plt.savefig('figures/Elimination_2_score.png')\n",
    "\n",
    "    with open('picklefiles/' + filename + '_wordsort.pickle', 'wb') as handle:\n",
    "        pickle.dump((s_keep, f_keep, mn_keep, f_nm_keep, s_go, f_go, mn_go, f_nm_go, s_mid, f_mid, mn_mid, f_nm_mid), handle)  \n",
    "        \n",
    "    return (s_keep, f_keep, mn_keep, f_nm_keep, s_go, f_go, mn_go, f_nm_go, s_mid, f_mid, mn_mid, f_nm_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (scores, fivescores, md_temp, nmd_temp, distdiff_temp, \n",
    "#          index_track, feat_names, filename) = elimround(prs_x_df, prs_x, prs_y, prs_dinfo, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sbest = 0.7 \n",
    "# fbest = 0.757142857143 \n",
    "# pbest = 0.65991213115\n",
    "# npbest = 3.5691193934\n",
    "# bmdist=0.90920726225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot([sbest, sbest], [0.75, 0.85])\n",
    "# plt.plot([0.58, 0.7], [fbest, fbest])\n",
    "# sc_r=[s+0.00*random.random() for s in scores]\n",
    "# fs_r=[f+0.00*random.random() for f in fivescores]\n",
    "# plt.scatter(sc_r, fs_r)\n",
    "# plt.xlabel('% top rank', fontsize=18)\n",
    "# plt.ylabel('% top five', fontsize=18)\n",
    "# plt.show()\n",
    "# plt.savefig('figures/Elimination_2_score.pdf')\n",
    "# plt.savefig('figures/Elimination_2_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot([pbest, pbest], [0.75, 0.85])\n",
    "# plt.plot([0.58, 0.7], [bmdist, bmdist])\n",
    "# sc_r=[s+0.0*random.random() for s in scores]\n",
    "# dd_r=[distdiff_temp]\n",
    "# plt.scatter(sc_r, dd_r)\n",
    "# plt.xlabel('% top rank', fontsize=18)\n",
    "# plt.ylabel('distance mean', fontsize=18)\n",
    "# plt.show()\n",
    "# plt.savefig('figures/Elimination_2_distance.pdf')\n",
    "# plt.savefig('figures/Elimination_2_distance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (s_keep, f_keep, mn_keep, f_nm_keep, s_go, f_go, \n",
    "#  mn_go, f_nm_go, s_mid, f_mid, mn_mid, f_nm_mid) = keeporgo(scores, fivescores, distdiff_temp, feat_names, \n",
    "#                                                             sbest, fbest, bmdist, 0, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
